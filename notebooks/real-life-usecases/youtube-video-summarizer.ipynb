{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# YouTube Video Summarizer\n",
    "Create a YouTube Video Summarizer Using Whisper and LangChain.\n",
    "\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the digital era, the abundance of information can be overwhelming, and we often find ourselves scrambling to consume as much content as possible within our limited time. In this lesson, we will unveil a powerful solution to help you efficiently summarize YouTube videos using two cutting-edge tools: Whisper and LangChain."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Workflow\n",
    "<br/>\n",
    "<img src=\"../../images/youtube-video-summarizer.png\" alt=\"YouTube Video Summarizer Workflow\" style=\"width: 70%; height: auto;\"/>\n",
    "<br/>\n",
    "First, we download the youtube video we are interested in and transcribe it using Whisper. Then, we’ll proceed by creating summaries using two different approaches:\n",
    "\n",
    "1. First we use an existing summarization chain to generate the final summary, this will just summarizing the video.\n",
    "2. Then, we use another approach more step-by-step to generate a final summary formatted in bullet points, consisting in splitting the transcription into chunks, computing their embeddings, and preparing ad-hoc prompts. Thsi will generate a summarized answer to the questions we ask with repect to videos."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "_ = load_dotenv(find_dotenv())\n",
    "openai.api_type = os.environ.get(\"OPENAI_API_TYPE\")\n",
    "openai.api_base = os.environ.get(\"OPENAI_API_BASE\")\n",
    "openai.api_version = os.environ.get(\"OPENAI_API_VERSION\")\n",
    "openai.api_key = os.environ.get(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 1 - Building the summarizer\n",
    "Here, we will perform the follwoing:\n",
    "1. Download the YouTube video file.\n",
    "2. Transcribe the video using Whisper.\n",
    "3. Split the transcription into smaller chunks and create documents out of it.\n",
    "4. Summarize the documents using LangChain with three different approaches: `stuff`, `refine`, and `map_reduce`."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Download the YouTube video file\n",
    "The `download_mp4_from_youtube()` function will download the best quality mp4 video file from any YouTube link and save it to the specified path and filename. We just need to copy/paste the selected video’s URL and pass it to mentioned function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    }
   ],
   "source": [
    "import yt_dlp\n",
    "\n",
    "filename = \"video.mp4\"\n",
    "\n",
    "\n",
    "def download_mp4_from_youtube(url):\n",
    "    # Set the options for the download\n",
    "    ydl_opts = {\n",
    "        \"format\": \"bestvideo[ext=mp4]+bestaudio[ext=m4a]/best[ext=mp4]\",\n",
    "        \"outtmpl\": filename,\n",
    "        \"quiet\": True,\n",
    "    }\n",
    "\n",
    "    # Download the video file\n",
    "    with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
    "        result = ydl.extract_info(url, download=True)\n",
    "\n",
    "\n",
    "url = \"https://www.youtube.com/watch?v=--khbXchTeE\"\n",
    "download_mp4_from_youtube(url)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Transcribe the video using Whisper\n",
    "The whisper package that we installed earlier provides the .load_model() method to download the model and transcribe a video file. Multiple different models are available: `tiny`, `base`, `small`, `medium`, and `large`. Each one of them has tradeoffs between accuracy and speed. We will use the `base` model for this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import whisper\n",
    "\n",
    "model = whisper.load_model(\"base\")\n",
    "result = model.transcribe(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " GPT-4 takes what you prompt it with and just runs with it. From one perspective, it's a tool. A thing you can use to get useful tasks done in language. From another perspective, it's a system that can make dreams, thoughts, ideas, flourish in text in front of you. GPT-4 is incredibly advanced and sophisticated. It can take in and generate up to 25,000 words of text around eight times more than chat GPT. It understands images and can express logical ideas about them. For example, it can tell us that if the strings in this image were cut, the balloons would fly away. This is the place where you just get turbocharged by these AIs. They're not perfect. They make mistakes, and so you really need to make sure that you know the work is being done to your level of expectation. But I think that it is fundamentally about amplifying what every person is able to do. GPT-4 training finished last August, and everything that's been happening in the past few months up until we've released it has been a giant sprint make it see for more aligned and also more useful. We have put in already a lot of internal guardrails around things like adversarial usage, unwanted content, and private-seek concerns. And when we release a model, we know things are not done. We know we have to learn, we know we have to update, we know we have to keep improving all the systems around it to make it suitable for society. To me, the most compelling use cases of these technologies will come from starting with a real human need. The obvious one where the systems have really incredible potential is in education. GPT-4 can teach a huge range of subjects. Imagine giving a fifth grader a personal math tutor with unlimited time and patience. It's a great tool to bring learning to everyone in a way that is personalized to their skill level. GPT-4 brings the dream of having the most useful, helpful, assistant to life. It's really about adding as much value to everyday life as possible. The partnership that OpenAI has with Microsoft is to shape this technology into something that's going to be useful for the world. The power of AI, hopefully, is that it can help us be more productive, which ultimately leads to better quality of life. The development of the transistor of the computer of the internet, the semiconductor industry, all the programming languages, everything came together to produce AI technology. And while it is very limited, it is already easy to imagine what the impact of a successor managed generations down the line will look like. We think that GPT-4 will be the world's first experience with a highly capable and advanced AI system. So we really care about this model being useful to everyone, not just the early adopters or people very close to technology. So it is really important to us that as many people as possible participate so that we can learn more about how it can be helpful to everyone. you\n"
     ]
    }
   ],
   "source": [
    "print(result[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We’ve got the result in the form of a raw text and it is possible to save it to a text file.\n",
    "folder_path = \"../../data\"\n",
    "file_path = os.path.join(folder_path, \"yt-video-text.txt\")\n",
    "with open(file_path, \"w\") as file:\n",
    "    file.write(result[\"text\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Text Splitting\n",
    "This ensures that the input text is broken down into manageable pieces, allowing for efficient processing by the language model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=800, chunk_overlap=20, separators=[\" \", \",\", \"\\n\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.docstore.document import Document\n",
    "\n",
    "with open(file_path) as f:\n",
    "    text = f.read()\n",
    "\n",
    "texts = text_splitter.split_text(text)\n",
    "docs = [Document(page_content=t) for t in texts[:4]]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Summarization with LangChain"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1 Summarization using default prompt with summarization chain"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `textwrap` library in Python provides a convenient way to wrap and format plain text by adjusting line breaks in an input paragraph. It is particularly useful when displaying text within a limited width, such as in console outputs, emails, or other formatted text displays. The library includes convenience functions like `wrap`, `fill`, and `shorten`, as well as the `TextWrapper` class that handles most of the work.\n",
    "\n",
    "> Note: We are using the `map_reduce` technique below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT-4 is an advanced AI tool designed to perform language tasks, generate creative ideas, and\n",
      "understand images. Developed with a focus on amplifying human capabilities and addressing concerns\n",
      "like adversarial usage and privacy, it has the potential to revolutionize education and enhance\n",
      "productivity. OpenAI's partnership with Microsoft aims to make GPT-4 globally useful and accessible\n",
      "to everyone, emphasizing the importance of involving many people in its development for widespread\n",
      "benefits.\n"
     ]
    }
   ],
   "source": [
    "from langchain.chat_models import AzureChatOpenAI\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "import textwrap\n",
    "\n",
    "\n",
    "llm = AzureChatOpenAI(deployment_name=\"gpt4\", temperature=0)\n",
    "\n",
    "chain = load_summarize_chain(llm, chain_type=\"map_reduce\")\n",
    "\n",
    "output_summary = chain.run(docs)\n",
    "wrapped_text = textwrap.fill(output_summary, width=100)\n",
    "print(wrapped_text)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the following line of code, we can see the prompt template that is used with the map_reduce technique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write a concise summary of the following:\n",
      "\n",
      "\n",
      "\"{text}\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\n"
     ]
    }
   ],
   "source": [
    "print(chain.llm_chain.prompt.template)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `refine` summarization chain is a method for generating more accurate and context-aware summaries. This chain type is designed to iteratively refine the summary by providing additional context when needed. That means: it generates the summary of the first chunk. Then, for each successive chunk, the work-in-progress summary is integrated with new info from the new chunk.\n",
    "\n",
    "> Note: We are using the `refine` technique below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT-4 is an advanced AI tool designed to amplify human capabilities in various fields, including\n",
      "education. Capable of performing useful language tasks, generating creative ideas in text,\n",
      "processing up to 25,000 words, understanding images, and expressing logical ideas, GPT-4 can serve\n",
      "as a personalized tutor for a wide range of subjects. Developed in partnership with Microsoft, its\n",
      "creators aim to shape this technology into a valuable tool for the world, enhancing productivity and\n",
      "ultimately improving the quality of life. With a vision of making GPT-4 accessible and useful to\n",
      "everyone, not just early adopters or tech-savvy individuals, its developers are committed to\n",
      "continuous improvement to make it suitable for society. The most compelling use cases for this\n",
      "technology will stem from addressing genuine human needs.\n"
     ]
    }
   ],
   "source": [
    "chain = load_summarize_chain(llm, chain_type=\"refine\")\n",
    "\n",
    "output_summary = chain.run(docs)\n",
    "wrapped_text = textwrap.fill(output_summary, width=100)\n",
    "print(wrapped_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir(chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your job is to produce a final summary\n",
      "We have provided an existing summary up to a certain point: {existing_answer}\n",
      "We have the opportunity to refine the existing summary(only if needed) with some more context below.\n",
      "------------\n",
      "{text}\n",
      "------------\n",
      "Given the new context, refine the original summary\n",
      "If the context isn't useful, return the original summary.\n"
     ]
    }
   ],
   "source": [
    "print(chain.refine_llm_chain.prompt.template)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2 Summarization using custom prompt with summarization chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "\n",
    "prompt_template = \"\"\"Write a concise bullet point summary of the following:\n",
    "\n",
    "{text}\n",
    "\n",
    "CONSCISE SUMMARY IN BULLET POINTS:\"\"\"\n",
    "\n",
    "BULLET_POINT_PROMPT = PromptTemplate(template=prompt_template, input_variables=[\"text\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Note: We are using the `stuff` technique below, which is the simplest and most naive approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- GPT-4 is an advanced AI tool for language tasks and generating ideas\n",
      "- Can generate up to 25,000 words, eight times more than ChatGPT\n",
      "- Understands images and can express logical ideas about them\n",
      "- Not perfect, requires user to ensure work meets expectations\n",
      "- Training completed in August, ongoing improvements for alignment and usefulness\n",
      "- Internal guardrails for adversarial usage, unwanted content, and privacy concerns\n",
      "- Potential for significant impact in education, personalized learning\n",
      "- OpenAI and Microsoft partnership to shape technology for global usefulness\n",
      "- AI advancements contribute to productivity and quality of life\n",
      "- GPT-4 aims to be accessible and helpful to a wide range of users\n"
     ]
    }
   ],
   "source": [
    "chain = load_summarize_chain(llm, chain_type=\"stuff\", prompt=BULLET_POINT_PROMPT)\n",
    "\n",
    "output_summary = chain.run(docs)\n",
    "\n",
    "wrapped_text = textwrap.fill(\n",
    "    output_summary, width=1000, break_long_words=False, replace_whitespace=False\n",
    ")\n",
    "print(wrapped_text)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 2 - Building the QnA summarizer\n",
    "Here, we will perform the follwoing:\n",
    "1. Download the YouTube video file.\n",
    "2. Transcribe the video using Whisper.\n",
    "3. Split the transcription into smaller chunks and create documents out of it.\n",
    "4. Add the documents to vector store and compute embeddings.\n",
    "4. Asking questions and generating summarized answers."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Download the YouTube video file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    }
   ],
   "source": [
    "import yt_dlp\n",
    "from typing import List, Tuple\n",
    "\n",
    "\n",
    "def download_mp4_from_youtube(\n",
    "    urls: List[str], job_id: int\n",
    ") -> List[Tuple[str, str, str]]:\n",
    "    # This will hold the titles and authors of each downloaded video\n",
    "    video_info = []\n",
    "\n",
    "    for i, url in enumerate(urls):\n",
    "        # Set the options for the download\n",
    "        file_temp = f\"./{job_id}_{i}.mp4\"\n",
    "        ydl_opts = {\n",
    "            \"format\": \"bestvideo[ext=mp4]+bestaudio[ext=m4a]/best[ext=mp4]\",\n",
    "            \"outtmpl\": file_temp,\n",
    "            \"quiet\": True,\n",
    "        }\n",
    "\n",
    "        # Download the video file\n",
    "        with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
    "            result = ydl.extract_info(url, download=True)\n",
    "            title = result.get(\"title\", \"\")\n",
    "            author = result.get(\"uploader\", \"\")\n",
    "\n",
    "        # Add the title and author to our list\n",
    "        video_info.append((file_temp, title, author))\n",
    "\n",
    "    return video_info\n",
    "\n",
    "\n",
    "urls = [\n",
    "    \"https://www.youtube.com/watch?v=--khbXchTeE\",\n",
    "    \"https://www.youtube.com/watch?v=qTgPSKKjfVg\",\n",
    "]\n",
    "videos_details = download_mp4_from_youtube(urls, 1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Transcribe the video using Whisper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import whisper\n",
    "\n",
    "text_separator = \"\\n\\n\"\n",
    "\n",
    "# load the model\n",
    "model = whisper.load_model(\"base\")\n",
    "\n",
    "# iterate through each video and transcribe\n",
    "results = \"\"\n",
    "for video in videos_details:\n",
    "    result = model.transcribe(video[0])\n",
    "    results += (result[\"text\"]) + text_separator\n",
    "    print(f\"Transcription for {video[0]}:\\n{result['text']}\\n\")\n",
    "\n",
    "file_path = os.path.join(folder_path, \"yt-multiple-text.txt\")\n",
    "with open(file_path, \"w\") as file:\n",
    "    file.write(results)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Text Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=800, chunk_overlap=20, separators=[text_separator, \" \", \",\", \"\\n\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.docstore.document import Document\n",
    "\n",
    "with open(file_path) as f:\n",
    "    text = f.read()\n",
    "\n",
    "texts = text_splitter.split_text(text)\n",
    "docs = [Document(page_content=t) for t in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content=\"GPT-4 takes what you prompt it with and just runs with it. From one perspective, it's a tool. A thing you can use to get useful tasks done in language. From another perspective, it's a system that can make dreams, thoughts, ideas, flourish in text in front of you. GPT-4 is incredibly advanced and sophisticated. It can take in and generate up to 25,000 words of text around eight times more than chat GPT. It understands images and can express logical ideas about them. For example, it can tell us that if the strings in this image were cut, the balloons would fly away. This is the place where you just get turbocharged by these AIs. They're not perfect. They make mistakes, and so you really need to make sure that you know the work is being done to your level of expectation. But I think that it\", metadata={}),\n",
       " Document(page_content=\"But I think that it is fundamentally about amplifying what every person is able to do. GPT-4 training finished last August, and everything that's been happening in the past few months up until we've released it has been a giant sprint make it see for more aligned and also more useful. We have put in already a lot of internal guardrails around things like adversarial usage, unwanted content, and private-seek concerns. And when we release a model, we know things are not done. We know we have to learn, we know we have to update, we know we have to keep improving all the systems around it to make it suitable for society. To me, the most compelling use cases of these technologies will come from starting with a real human need. The obvious one where the systems have really incredible potential\", metadata={}),\n",
       " Document(page_content=\"potential is in education. GPT-4 can teach a huge range of subjects. Imagine giving a fifth grader a personal math tutor with unlimited time and patience. It's a great tool to bring learning to everyone in a way that is personalized to their skill level. GPT-4 brings the dream of having the most useful, helpful, assistant to life. It's really about adding as much value to everyday life as possible. The partnership that OpenAI has with Microsoft is to shape this technology into something that's going to be useful for the world. The power of AI, hopefully, is that it can help us be more productive, which ultimately leads to better quality of life. The development of the transistor of the computer of the internet, the semiconductor industry, all the programming languages, everything came\", metadata={}),\n",
       " Document(page_content=\"everything came together to produce AI technology. And while it is very limited, it is already easy to imagine what the impact of a successor managed generations down the line will look like. We think that GPT-4 will be the world's first experience with a highly capable and advanced AI system. So we really care about this model being useful to everyone, not just the early adopters or people very close to technology. So it is really important to us that as many people as possible participate so that we can learn more about how it can be helpful to everyone. you\", metadata={}),\n",
       " Document(page_content=\"Have you ever seen a polar bear playing bass? Or robot painted like a Picasso? Didn't think so. Dolly too is a new AI system from OpenAI that can take simple text descriptions like a Koala-Dunking Abaskable and turn them into photo-realistic images that have never existed before. Dolly too can also realistically edit and retouch photos. Based on a simple natural language description, it can fill in or replace part of an image with AI-generated imagery that blends seamlessly with the original. It's called in-painting. In January 2021, OpenAI introduced Dolly, a system that could generate images from text, like this avocado armchair. Dolly too takes the technology even further with higher resolution, greater comprehension, and new capabilities, like in-painting. It can even start with an\", metadata={}),\n",
       " Document(page_content=\"even start with an image as an input and create variations with different angles and styles. Dolly was created by training a neural network on images and their text descriptions. Through deep learning, it not only understands individual objects, like koala bears and motorcycles, but learns from relationships between objects. And when you ask Dolly for an image of a koala bear riding a motorcycle, it knows how to create that or anything else with a relationship to another object or action. The Dolly research has three main outcomes. First, it can help people express themselves visually in ways they may not have been able to before. Second, an AI-generated image can tell us a lot about whether the system understands us or is just repeating what it's been taught. Third, Dolly helps humans\", metadata={}),\n",
       " Document(page_content=\"Dolly helps humans understand how AI systems see and understand our world. This is a critical part of developing AI that's useful and safe. The technology is constantly evolving, and Dolly too has limitations. If it's taught with images that are incorrectly labeled, like a plain labeled car, and a user tries to generate a car, Dolly may create a plane. It's like talking to a person who learned the wrong word for something. Dolly can also be limited by gaps in its training. If you type that boon and Dolly has learned what a bad boon is through images and accurate labels, it will generate a lot of great bad boons. But if you type howler monkey, and it hasn't learned what a howler monkey is, Dolly will give you its best idea of what it thinks it could be, like a howling monkey. What's\", metadata={}),\n",
       " Document(page_content=\"monkey. What's exciting about the approach used to train Dolly is that it can take what it learned from a variety of other labeled images and then apply it to the other object. And then apply it to a new image. Given a picture of a monkey, Dolly can infer what it would look like doing something it's never done before, like paying its taxes while wearing a funny hat. Dolly is an example of how imaginative humans and clever systems can work together to make new things, amplifying our creative potential.\", metadata={})]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Vector Store with Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import DeepLake\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings()\n",
    "\n",
    "# create Deep Lake dataset\n",
    "# TODO: use your organization id here. (by default, org id is your username)\n",
    "my_activeloop_org_id = os.environ.get(\"ACTIVELOOP_ORG_ID\")\n",
    "my_activeloop_dataset_name = \"youtube_summarizer\"\n",
    "dataset_path = f\"hub://{my_activeloop_org_id}/{my_activeloop_dataset_name}\"\n",
    "\n",
    "db = DeepLake(dataset_path=dataset_path, embedding_function=embeddings)\n",
    "db.add_documents(docs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Asking questions and generating summarized answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "prompt_template = \"\"\"Use the following pieces of transcripts from a video to \\\n",
    "answer the question in bullet points and summarized. If you don't know the answer, \\\n",
    "just say that you don't know, don't try to make up an answer.\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "Summarized answer in bullet points:\"\"\"\n",
    "\n",
    "PROMPT = PromptTemplate(\n",
    "    template=prompt_template, input_variables=[\"context\", \"question\"]\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's configure the retriever object. The `distance_metric` determines how the `Retriever` measures \"distance\" or similarity between different data points in the database. By setting `distance_metric` to `cos`, the `Retriever` will use cosine similarity as its distance metric. Also, by setting `k` to `4`, the `Retriever` will return the 4 most similar or closest results according to the distance metric when a search is performed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = db.as_retriever()\n",
    "retriever.search_kwargs[\"distance_metric\"] = \"cos\"\n",
    "retriever.search_kwargs[\"k\"] = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Dall-E is an AI system that creates images based on text descriptions\n",
      "- It was created by training a neural network on images and their text descriptions\n",
      "- Understands relationships between objects and actions\n",
      "- Helps people express themselves visually and amplifies creative potential\n",
      "- Creator: OpenAI\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "chain_type_kwargs = {\"prompt\": PROMPT}\n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",  # Feel free to experiment with different chain types to see which one works best for you.\n",
    "    retriever=retriever,\n",
    "    chain_type_kwargs=chain_type_kwargs,\n",
    ")\n",
    "\n",
    "print(qa.run(\"Summarize dall e in simple terms and who is its creator?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- GPT-4 and DALL-E are not the same thing; they are different AI systems.\n",
      "- Both are created by OpenAI, so their creators are the same.\n"
     ]
    }
   ],
   "source": [
    "response = qa.run(\"Are gpt4 and dall e the same thing? Are their creators the same?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Don't know\n"
     ]
    }
   ],
   "source": [
    "response = qa.run(\"Which was released first, gpt4 or dall e?\")\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
