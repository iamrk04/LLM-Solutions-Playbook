{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Codebase Understanding using LLM\n",
    "Explore how LangChain, Deep Lake, and GPT-4 can transform our understanding of complex codebases, such as Twitter's open-sourced recommendation algorithm.\n",
    "\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "In this lesson we will explore how LangChain, Deep Lake, and GPT-4 can transform our understanding of complex codebases, such as Twitter's open-sourced recommendation algorithm. This approach enables us to ask any question directly to the source code, significantly speeding up the code comprehension.\n",
    "\n",
    "In this lesson, you'll learn how to index a codebase, store embeddings and code in Deep Lake, set up a Conversational Retriever Chain, and ask insightful questions to the codebase."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Workflow"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This guide involves understanding source code using LangChain in three steps:\n",
    "1. Index a codebase by cloning the repository, parsing the code, dividing it into chunks, and using OpenAI/HuggingFace to perform indexing.\n",
    "2. Establish a Conversational Retriever Chain by loading the dataset, setting up the retriever, and connecting to a language model like GPT-4 for question answering.\n",
    "3. Query the codebase in natural language and retrieve answers. The guide ends with a demonstration of how to ask and retrieve answers to several questions about the indexed codebase."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "_ = load_dotenv(find_dotenv())\n",
    "openai.api_type = os.environ.get(\"OPENAI_API_TYPE\")\n",
    "openai.api_base = os.environ.get(\"OPENAI_API_BASE\")\n",
    "openai.api_version = os.environ.get(\"OPENAI_API_VERSION\")\n",
    "openai.api_key = os.environ.get(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the system"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Indexing the Twitter Algorithm Code Base (Optional)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can skip this part and jump right into using an already indexed dataset (just like the one in this example). To index the code base, first clone the repository, parse the code, break it into chunks, and apply OpenAI/HuggingFace indexing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/twitter/the-algorithm ../../temp/the-algorithm"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, load all files inside the repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain.document_loaders import TextLoader\n",
    "\n",
    "root_dir = \"../../temp/the-algorithm\"\n",
    "docs = []\n",
    "for dirpath, dirnames, filenames in os.walk(root_dir):\n",
    "    for file in filenames:\n",
    "        try:\n",
    "            loader = TextLoader(os.path.join(dirpath, file), encoding=\"utf-8\")\n",
    "            docs.extend(loader.load_and_split())\n",
    "        except Exception as e:\n",
    "            pass"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Subsequently, divide the loaded files into chunks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "texts = text_splitter.split_documents(docs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform the indexing process. It can take large amount of time to calculate embeddings and upload them to Activeloop. Afterward, you can publish the dataset publicly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import DeepLake\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "my_activeloop_org_id = os.environ.get(\"ACTIVELOOP_ORG_ID\")\n",
    "my_activeloop_dataset_name = \"twitter-algorithm\"\n",
    "\n",
    "dataset_path = f\"hub://{my_activeloop_org_id}/{my_activeloop_dataset_name}\"\n",
    "db = DeepLake(dataset_path=dataset_path, embedding_function=HuggingFaceEmbeddings())\n",
    "db.add_documents(texts)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the dataset has been already created, you can load it later without recomputing embeddings as seen below."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Conversational Retriever Chain"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, load the dataset, establish the retriever, and create the Conversational Chain:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deep Lake Dataset in hub://iamrk04/twitter-algorithm already exists, loading from the storage\n"
     ]
    }
   ],
   "source": [
    "from langchain.vectorstores import DeepLake\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "my_activeloop_org_id = os.environ.get(\"ACTIVELOOP_ORG_ID\")\n",
    "# my_activeloop_org_id = \"davitbun\" # contains the already embedded dataset\n",
    "my_activeloop_dataset_name = \"twitter-algorithm\"\n",
    "\n",
    "dataset_path = f\"hub://{my_activeloop_org_id}/{my_activeloop_dataset_name}\"\n",
    "\n",
    "db = DeepLake(\n",
    "    dataset_path=dataset_path,\n",
    "    read_only=True,\n",
    "    embedding_function=HuggingFaceEmbeddings(),\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, setup the retriever:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = db.as_retriever()\n",
    "retriever.search_kwargs[\"distance_metric\"] = \"cos\"\n",
    "retriever.search_kwargs[\"fetch_k\"] = 100\n",
    "retriever.search_kwargs[\"maximal_marginal_relevance\"] = True\n",
    "retriever.search_kwargs[\"k\"] = 10"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also define custom filtering functions using `Deep Lake filters`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter(x):\n",
    "    if \"com.google\" in x[\"text\"].data()[\"value\"]:\n",
    "        return False\n",
    "    metadata = x[\"metadata\"].data()[\"value\"]\n",
    "    return \"scala\" in metadata[\"source\"] or \"py\" in metadata[\"source\"]\n",
    "\n",
    "\n",
    "# Uncomment the following line to apply custom filtering\n",
    "# retriever.search_kwargs['filter'] = filter"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Connect to GPT-4 for question answering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import AzureChatOpenAI\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "\n",
    "model = AzureChatOpenAI(deployment_name=\"gpt4\", temperature=0.1)\n",
    "qa = ConversationalRetrievalChain.from_llm(model, retriever=retriever)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Ask Questions to the Codebase in Natural Language"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define all the juicy questions you want to be answered:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> **Question**: What does favCountParams do? \n",
      "\n",
      "**Answer**: I couldn't find any information about \"favCountParams\" in the provided context. Please provide more information or context about \"favCountParams\" to help me understand its purpose and functionality. \n",
      "\n",
      "-> **Question**: is it Likes + Bookmarks, or not clear from the code? \n",
      "\n",
      "**Answer**: It is not clear from the code provided whether favCountParams is the sum of Likes and Bookmarks. \n",
      "\n",
      "-> **Question**: What are the major negative modifiers that lower your linear ranking parameters? \n",
      "\n",
      "**Answer**: Based on the provided context, there are a few negative modifiers that can lower linear ranking parameters:\n",
      "\n",
      "1. Fatigue Factor: In the `ImpressionBasedFatigueRanker` class, the fatigue factor is used to penalize candidates based on their recent impressions. The higher the fatigue factor, the more the candidate's rank is penalized due to recent impressions.\n",
      "\n",
      "2. AuthorSensitiveScoreWeightInReranking: This parameter is used to adjust the weight of the author's sensitive score in reranking. A negative value for this parameter can lower the ranking of candidates with higher sensitive scores (e.g., nudity rate).\n",
      "\n",
      "3. QualityUprankingDownboostForLowQualityProducersParam: This parameter is used to downboost low-quality producers in the ranking process. A value less than 1.0 can lower the ranking of candidates from low-quality producers.\n",
      "\n",
      "4. QualityUprankingBoostForHeavyRankingParam and QualityUprankingSigmoidWeightForHeavyRankingParam: These parameters are used in the quality upranking process for heavy ranking. Negative values for these parameters can lower the ranking of candidates based on their quality scores.\n",
      "\n",
      "Please note that these are just a few examples of negative modifiers that can lower linear ranking parameters. There might be other factors in the ranking process that can also negatively impact the ranking of candidates. \n",
      "\n",
      "-> **Question**: How does Heavy ranker work. what are it’s main inputs? \n",
      "\n",
      "**Answer**: The Heavy Ranker is a core component of Twitter's notification recommendation system. It is a multi-task learning model that predicts the probabilities of target users opening and engaging with the sent notifications. The recommendation system consists of four major components: 1) candidate generation, 2) light ranking, 3) heavy ranking, and 4) quality control.\n",
      "\n",
      "The main inputs for the Heavy Ranker are:\n",
      "\n",
      "1. User mass TSV file: This file is generated by the twadoop user_mass job and contains information about the user's mass, which is used in the PageRank algorithm.\n",
      "2. Graph data: This can be a TSV file or a combination of flock edges and real graph inputs for weights. The graph data represents the relationships between users and is used to calculate the weighted PageRank.\n",
      "3. Initial PageRank: This is the starting point for the PageRank computation, generated from the graph data.\n",
      "4. Features: The Heavy Ranker uses features generated by the Timelines Aggregation Jobs, which include both batch aggregate jobs and real-time aggregate jobs. These features are used to train the model and make predictions.\n",
      "\n",
      "The Heavy Ranker processes these inputs and calculates the probabilities of users engaging with the notifications. Based on these probabilities, the system ranks and selects the most relevant notifications to send to the users. \n",
      "\n",
      "-> **Question**: Do you need to follow different strategies to get most followers vs to get most likes and bookmarks per tweet? \n",
      "\n",
      "**Answer**: Yes, it is necessary to employ different strategies to maximize the number of followers compared to maximizing the number of likes and bookmarks per tweet. These are different objectives and require different approaches.\n",
      "\n",
      "Maximizing the number of followers typically involves creating a consistent and engaging presence on the platform, sharing valuable content, interacting with other users, and promoting your account through various channels. This helps to build a loyal and engaged audience who will follow your account.\n",
      "\n",
      "On the other hand, maximizing the number of likes and bookmarks per tweet focuses on creating high-quality, engaging, and shareable content that resonates with your target audience. This may involve optimizing the content, timing, and format of your tweets, as well as using relevant hashtags and mentions to increase visibility.\n",
      "\n",
      "While there may be some overlap in the strategies used to achieve these objectives, it is important to recognize that they are distinct goals and require tailored approaches to be successful. \n",
      "\n",
      "-> **Question**: What are some unexpected fingerprints for spam factors? \n",
      "\n",
      "**Answer**: Some unexpected factors that can be considered as fingerprints for spam include:\n",
      "\n",
      "1. Unusual patterns of engagement: Fake retweets, replies, favorites, and quote counts can be indicators of spam activity.\n",
      "\n",
      "2. Spammy or compromised user labels: Users with labels such as \"Compromised,\" \"EngagementSpammer,\" \"LowQuality,\" \"ReadOnly,\" or \"SpamHighRecall\" can be considered as potential sources of spam.\n",
      "\n",
      "3. High volume of abuse reports: A high number of abuse reports against a user or their tweets can be a sign of spammy behavior.\n",
      "\n",
      "4. Spammy content scores: Machine learning models can be used to analyze tweet content and assign a spam score, which can be used as a fingerprint for spam.\n",
      "\n",
      "5. Unusual URL patterns: Spam tweets may contain shortened or obfuscated URLs that lead to malicious websites or are used for phishing purposes.\n",
      "\n",
      "6. Rapid account creation: Accounts created in quick succession or with similar patterns in usernames, email addresses, or profile information can be considered as potential spam accounts.\n",
      "\n",
      "7. High volume of tweets in a short period: Users who post a large number of tweets in a short time frame, especially with similar content or links, can be considered as potential spammers.\n",
      "\n",
      "8. Unusual follower/following ratios: Accounts with a high number of followers but a low number of following or vice versa can be an indicator of spam activity.\n",
      "\n",
      "9. Inconsistencies in language or content: Spam tweets may contain inconsistent language, grammar, or content that does not match the user's profile or typical behavior.\n",
      "\n",
      "10. Use of automation or bots: Accounts that show signs of automation, such as posting at regular intervals or using specific keywords or hashtags repeatedly, can be considered as potential sources of spam. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "questions = [\n",
    "    \"What does favCountParams do?\",\n",
    "    \"is it Likes + Bookmarks, or not clear from the code?\",\n",
    "    \"What are the major negative modifiers that lower your linear ranking parameters?\",\n",
    "    # \"How do you get assigned to SimClusters?\",\n",
    "    # \"What is needed to migrate from one SimClusters to another SimClusters?\",\n",
    "    # \"How much do I get boosted within my cluster?\",\n",
    "    \"How does Heavy ranker work. what are it’s main inputs?\",\n",
    "    # \"How can one influence Heavy ranker?\",\n",
    "    # \"why threads and long tweets do so well on the platform?\",\n",
    "    # \"Are thread and long tweet creators building a following that reacts to only threads?\",\n",
    "    \"Do you need to follow different strategies to get most followers vs to get most likes and bookmarks per tweet?\",\n",
    "    # \"Content meta data and how it impacts virality (e.g. ALT in images).\",\n",
    "    \"What are some unexpected fingerprints for spam factors?\",\n",
    "    # \"Is there any difference between company verified checkmarks and blue verified individual checkmarks?\",\n",
    "]\n",
    "chat_history = []\n",
    "\n",
    "for question in questions:\n",
    "    result = qa({\"question\": question, \"chat_history\": chat_history})\n",
    "    chat_history.append((question, result[\"answer\"]))\n",
    "    print(f\"-> **Question**: {question} \\n\")\n",
    "    print(f\"**Answer**: {result['answer']} \\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
