{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Voice Assistant for Knowledge Base\n",
    "Build your own voice assistant for your knowledge base using Whisper.\n",
    "\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is how the system will work:\n",
    "1. Record our voice input, which is the user query.\n",
    "2. Transcribe the voice input into text using Whisper.\n",
    "3. Use the `RetrievalQA` chain to retrieve the answer from the knowledge base using LLM.\n",
    "4. Convert the answer into voice output and play it.\n",
    "\n",
    "The core of the project revolves around a robust question-answering mechanism. This process initiates with loading the vector database, a repository housing several documents relevant to our potential queries. On posing a question, the system retrieves the documents from this database and, along with the question, feeds them to the LLM. The LLM then generates the response based on retrieved documents."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "_ = load_dotenv(find_dotenv())\n",
    "openai.api_type = os.environ.get(\"OPENAI_API_TYPE\")\n",
    "openai.api_base = os.environ.get(\"OPENAI_API_BASE\")\n",
    "openai.api_version = os.environ.get(\"OPENAI_API_VERSION\")\n",
    "openai.api_key = os.environ.get(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the system"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Getting the data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 Sourcing content from HF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "\n",
    "def _get_documentation_urls() -> List[str]:\n",
    "    # List of relative URLs for Hugging Face documentation pages\n",
    "    return [\n",
    "        \"/docs/huggingface_hub/guides/overview\",\n",
    "        \"/docs/huggingface_hub/guides/download\",\n",
    "        \"/docs/huggingface_hub/guides/upload\",\n",
    "        # '/docs/huggingface_hub/guides/hf_file_system',\n",
    "        # '/docs/huggingface_hub/guides/repository',\n",
    "        # '/docs/huggingface_hub/guides/search',\n",
    "        # You may add additional URLs here or replace all of them\n",
    "    ]\n",
    "\n",
    "\n",
    "def _construct_full_url(base_url: str, relative_url: str) -> str:\n",
    "    # Construct the full URL by appending the relative URL to the base URL\n",
    "    return base_url + relative_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "def _scrape_page_content(url: str) -> str:\n",
    "    # Send a GET request to the URL and parse the HTML response using BeautifulSoup\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    # Extract the desired content from the page (in this case, the body text)\n",
    "    text = soup.body.text.strip()\n",
    "    # Remove non-ASCII characters\n",
    "    text = re.sub(r\"[\\x00-\\x08\\x0b-\\x0c\\x0e-\\x1f\\x7f-\\xff]\", \"\", text)\n",
    "    # Remove extra whitespace and newlines\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    return text.strip()\n",
    "\n",
    "\n",
    "def _scrape_all_content(\n",
    "    base_url: str, relative_urls: List[str], file_path: str\n",
    ") -> List[str]:\n",
    "    # Loop through the list of URLs, scrape content and add it to the content list\n",
    "    content = []\n",
    "    for relative_url in relative_urls:\n",
    "        full_url = _construct_full_url(base_url, relative_url)\n",
    "        scraped_content = _scrape_page_content(full_url)\n",
    "        content.append(scraped_content.rstrip(\"\\n\"))\n",
    "\n",
    "    # Write the scraped content to a file\n",
    "    with open(file_path, \"w\", encoding=\"utf-8\") as file:\n",
    "        for item in content:\n",
    "            file.write(\"%s\\n\" % item)\n",
    "\n",
    "    return content"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 Loading and splitting texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.schema import Document\n",
    "import os\n",
    "\n",
    "# Define a function to load documents from a file\n",
    "def _load_docs(file_path) -> List[Document]:\n",
    "    # Create an empty list to hold the documents\n",
    "    docs = []\n",
    "    try:\n",
    "        # Load the file using the TextLoader class and UTF-8 encoding\n",
    "        loader = TextLoader(file_path, encoding=\"utf-8\")\n",
    "        # Split the loaded file into separate documents and add them to the list of documents\n",
    "        docs.extend(loader.load_and_split())\n",
    "    except Exception as e:\n",
    "        # If an error occurs during loading, ignore it and return an empty list of documents\n",
    "        pass\n",
    "    # Return the list of documents\n",
    "    return docs\n",
    "\n",
    "\n",
    "def _split_docs(docs: List[Document]) -> List[Document]:\n",
    "    text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "    return text_splitter.split_documents(docs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Embedding and storing in Deep Lake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import DeepLake\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "\n",
    "def create_knowledge_base(dataset_path: str) -> DeepLake:\n",
    "    \"\"\"\n",
    "    Creates a DeepLake database from the Hugging Face documentation.\n",
    "\n",
    "    :param dataset_path: The path for DeepLake database.\n",
    "    :return: A DeepLake database.\n",
    "    \"\"\"\n",
    "    base_url = \"https://huggingface.co\"\n",
    "    # Set the file_path to which the scraped content will be saved\n",
    "    file_path = \"../../temp/voice_assistant_kb.txt\"\n",
    "    relative_urls = _get_documentation_urls()\n",
    "    # Scrape all the content from the relative URLs and save it to the content file\n",
    "    content = _scrape_all_content(base_url, relative_urls, file_path)\n",
    "    # Load the content from the file\n",
    "    docs = _load_docs(file_path)\n",
    "    # Split the content into individual documents\n",
    "    docs = _split_docs(docs)\n",
    "    # Create a DeepLake database with the given dataset path and embedding function\n",
    "\n",
    "    db = DeepLake(dataset_path=dataset_path, embedding_function=HuggingFaceEmbeddings())\n",
    "    # Add the individual documents to the database\n",
    "    db.add_documents(docs)\n",
    "    # Clean up by deleting the content file\n",
    "    # os.remove(file_path)\n",
    "    return db"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Search db for answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import AzureChatOpenAI\n",
    "from langchain.chains import RetrievalQA\n",
    "from typing import Dict\n",
    "\n",
    "# Search the database for a response based on the user's query\n",
    "def get_LLM_response(user_input: str, db: DeepLake) -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Generates LLM response after searching the database for relevant info based on the user's query.\n",
    "\n",
    "    :param user_input: The user's query.\n",
    "    :param db: The DeepLake database.\n",
    "    :return: The LLM response.\n",
    "    \"\"\"\n",
    "    retriever = db.as_retriever()\n",
    "    retriever.search_kwargs[\"distance_metric\"] = \"cos\"\n",
    "    retriever.search_kwargs[\"fetch_k\"] = 100\n",
    "    retriever.search_kwargs[\"maximal_marginal_relevance\"] = True\n",
    "    retriever.search_kwargs[\"k\"] = 4\n",
    "    model = AzureChatOpenAI(deployment_name=\"gpt4\", temperature=0)\n",
    "    qa = RetrievalQA.from_llm(model, retriever=retriever, return_source_documents=True)\n",
    "\n",
    "    print(\"\\nQuerying LLM...\")\n",
    "    return qa(user_input)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Voice Assistant"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1 Record Audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sounddevice as sd\n",
    "from scipy.io.wavfile import write\n",
    "\n",
    "\n",
    "def record_and_save_audio(record_sec: int, file_path: str) -> None:\n",
    "    \"\"\"\n",
    "    Record audio for the given number of seconds and save it to the given file path.\n",
    "\n",
    "    :param record_sec: The number of seconds to record audio for.\n",
    "    :param file_path: The file path to which the recorded audio should be saved; must be a `.wav` file.\n",
    "    \"\"\"\n",
    "    fs = 44100  # Sample rate\n",
    "\n",
    "    print(f\"\\nRecording for {record_sec} seconds...\")\n",
    "    myrecording = sd.rec(int(record_sec * fs), samplerate=fs, channels=1)\n",
    "    sd.wait()  # Wait until recording is finished\n",
    "    print(\"Recording finished.\")\n",
    "\n",
    "    write(file_path, fs, myrecording)  # Save as WAV file\n",
    "    print(f\"Recording saved at {file_path}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2 Transcribe Recorded Audio (Speech to Text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import whisper\n",
    "\n",
    "# Transcribe audio using OpenAI Whisper API\n",
    "def transcribe_audio(audio_file_path: str) -> str:\n",
    "    \"\"\"\n",
    "    Converts the given audio file to text using the OpenAI Whisper API.\n",
    "\n",
    "    :param audio_file_path: The path to the audio file to be transcribed.\n",
    "    :return: The transcribed text.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        model = whisper.load_model(\"base\")\n",
    "\n",
    "        print(\"\\nTranscribing audio...\")\n",
    "        response = model.transcribe(audio_file_path, fp16=False)\n",
    "        print(\"Transcription complete.\")\n",
    "\n",
    "        print(f\"Transcribed text: {response['text']}\")\n",
    "\n",
    "        return response[\"text\"]\n",
    "    except Exception as e:\n",
    "        print(f\"Error calling Whisper API: {str(e)}\")\n",
    "        return None"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3 Play the response (Text to Speech)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gtts import gTTS\n",
    "import sounddevice as sd\n",
    "import soundfile as sf\n",
    "\n",
    "\n",
    "def text_to_speech_play(text: str, file_path: str, language: str = \"en\") -> None:\n",
    "    \"\"\"\n",
    "    Convert text to speech, save it to a file and then play it.\n",
    "\n",
    "    :param text: The text to convert to speech.\n",
    "    :param file_path: The path to the file to save the speech to.\n",
    "    :param language: The language of the text.\n",
    "    \"\"\"\n",
    "    myobj = gTTS(text=text, lang=language, slow=False)\n",
    "\n",
    "    # Saving the converted audio in a mp3 file named\n",
    "    myobj.save(file_path)\n",
    "    print(f\"\\nResponse as speech saved at {file_path}\")\n",
    "\n",
    "    # Playing the converted file\n",
    "    # Extract data and sampling rate from file\n",
    "    data, fs = sf.read(file_path, dtype=\"float32\")\n",
    "    sd.play(data, fs)\n",
    "    status = sd.wait()  # Wait until file is done playing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Putting it all together"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1 Get the Knowledge Base at VectorStore ready"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your Deep Lake dataset has been successfully created!\n",
      "The dataset is private so make sure you are logged in!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset(path='hub://iamrk04/voice_assistant_kb', tensors=['embedding', 'id', 'metadata', 'text'])\n",
      "\n",
      "  tensor      htype      shape     dtype  compression\n",
      "  -------    -------    -------   -------  ------- \n",
      " embedding  embedding  (10, 768)  float32   None   \n",
      "    id        text      (10, 1)     str     None   \n",
      " metadata     json      (10, 1)     str     None   \n",
      "   text       text      (10, 1)     str     None   \n"
     ]
    }
   ],
   "source": [
    "my_activeloop_org_id = os.environ.get(\"ACTIVELOOP_ORG_ID\")\n",
    "my_activeloop_dataset_name = \"voice_assistant_kb\"\n",
    "dataset_path = f\"hub://{my_activeloop_org_id}/{my_activeloop_dataset_name}\"\n",
    "db = create_knowledge_base(dataset_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2 Get the VectorStore handle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_embeddings_and_database(active_loop_data_set_path: str) -> DeepLake:\n",
    "    embeddings = HuggingFaceEmbeddings()\n",
    "    print(\"\\nLoading database...\")\n",
    "    db = DeepLake(\n",
    "        dataset_path=active_loop_data_set_path,\n",
    "        read_only=True,\n",
    "        embedding_function=embeddings,\n",
    "    )\n",
    "    print(\"Database loaded.\")\n",
    "    return db"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.3 The `main()` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "def main(record_sec: int = 10) -> None:\n",
    "    \"\"\"\n",
    "    The main function that runs the entire system.\n",
    "\n",
    "    :param record_sec: The number of seconds to record audio for.\n",
    "    \"\"\"\n",
    "    temp_folder = \"../../temp\"\n",
    "    os.makedirs(temp_folder, exist_ok=True)\n",
    "    record_audio_file_path = temp_folder + \"/record.wav\"\n",
    "    output_audio_file_path = temp_folder + \"/output.mp3\"\n",
    "\n",
    "    # record audio\n",
    "    record_and_save_audio(record_sec=record_sec, file_path=record_audio_file_path)\n",
    "\n",
    "    # transcribe audio\n",
    "    text = transcribe_audio(audio_file_path=record_audio_file_path)\n",
    "\n",
    "    # load embeddings and database\n",
    "    my_activeloop_org_id = os.environ.get(\"ACTIVELOOP_ORG_ID\")\n",
    "    my_activeloop_dataset_name = \"voice_assistant_kb\"\n",
    "    dataset_path = f\"hub://{my_activeloop_org_id}/{my_activeloop_dataset_name}\"\n",
    "    db = load_embeddings_and_database(active_loop_data_set_path=dataset_path)\n",
    "\n",
    "    # do similarity search\n",
    "    response = get_LLM_response(user_input=text, db=db)\n",
    "    print(f\"Response: {response}\")\n",
    "\n",
    "    result = response[\"result\"]\n",
    "\n",
    "    # text to speech\n",
    "    text_to_speech_play(text=result, file_path=output_audio_file_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Run the system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Recording for 8 seconds...\n",
      "Recording finished.\n",
      "Recording saved at ../../temp/record.wav\n",
      "\n",
      "Transcribing audio...\n",
      "Transcription complete.\n",
      "Transcribed text:  How do I search for model at hugging face hub?\n",
      "\n",
      "Loading database...\n",
      "Deep Lake Dataset in hub://iamrk04/voice_assistant_kb already exists, loading from the storage\n",
      "Database loaded.\n",
      "\n",
      "Querying LLM...\n",
      "Response: {'query': ' How do I search for model at hugging face hub?', 'result': 'To search for models on the Hugging Face Hub using the `huggingface_hub` library, you can use the `HfApi` class and its `search_models()` method. Here\\'s an example of how to search for models:\\n\\n```python\\nfrom huggingface_hub import HfApi\\n\\napi = HfApi()\\nmodels = api.search_models(\"bert\")\\n\\nfor model in models:\\n    print(model.modelId)\\n```\\n\\nReplace \"bert\" with the keyword or model name you want to search for. This will return a list of models matching your search query, and you can iterate through the results to display the model IDs.', 'source_documents': [Document(page_content='Hugging Face Models Datasets Spaces Docs Solutions Pricing Log In Sign Up Hub Python Library documentation Download files from the Hub Hub Python Library Search documentation mainv0.16.3v0.15.1v0.14.1v0.13.4v0.12.1v0.11.0v0.10.1v0.9.1v0.8.1v0.7.0.rc0v0.6.0.rc0v0.5.1 EN Get started Home Quickstart Installation How-to guides Overview Download files Upload files HfFileSystem Repository Search Inference Community Tab Cache Model Cards Manage your Space Integrate a library Webhooks server Conceptual guides Git vs HTTP paradigm Reference Overview Login and logout Environment variables Managing local and online repositories Hugging Face Hub API Downloading files Mixins & serialization methods Inference Client HfFileSystem Utilities Discussions and Pull Requests Cache-system reference Repo Cards and Repo Card Data Space runtime TensorBoard logger Webhooks server Join the Hugging Face community and get access to the augmented documentation experience Collaborate on models, datasets and Spaces Faster examples with accelerated inference Switch between documentation themes Sign Up to get started Download files from the Hub The huggingface_hub library provides functions to download files from the repositories stored on the Hub. You can use these functions independently or integrate them into your own library, making it more convenient for your users to interact with the Hub. This guide will show you how to: Download and cache a single file. Download and cache an entire repository. Download files to a local folder. Download a single file The hf_hub_download() function is the main function for downloading files from the Hub. It downloads the remote file, caches it on disk (in a version-aware way), and returns its local file path. The returned filepath is a pointer to the HF local cache. Therefore, it is important to not modify the file to avoid having a corrupted cache. If you are interested in getting to know more about how files are cached, please refer to our caching guide. From latest version Select the file to download using the repo_id, repo_type and filename parameters. By default, the file will be considered as being part of a model repo. Copied >>> from huggingface_hub import hf_hub_download >>> hf_hub_download(repo_id=\"lysandre/arxiv-nlp\", filename=\"config.json\") \\'/root/.cache/huggingface/hub/models--lysandre--arxiv-nlp/snapshots/894a9adde21d9a3e3843e6d5aeaaf01875c7fade/config.json\\' # Download from a dataset >>> hf_hub_download(repo_id=\"google/fleurs\", filename=\"fleurs.py\", repo_type=\"dataset\") \\'/root/.cache/huggingface/hub/datasets--google--fleurs/snapshots/199e4ae37915137c555b1765c01477c216287d34/fleurs.py\\' From specific version By default, the latest version from the main branch is downloaded. However, in some cases you want to download a file at a particular version (e.g. from a specific branch, a PR, a tag or a commit hash). To do so, use the revision parameter: Copied # Download from the `v1.0` tag >>> hf_hub_download(repo_id=\"lysandre/arxiv-nlp\", filename=\"config.json\", revision=\"v1.0\") # Download from the `test-branch` branch >>> hf_hub_download(repo_id=\"lysandre/arxiv-nlp\", filename=\"config.json\", revision=\"test-branch\") # Download from Pull Request #3 >>> hf_hub_download(repo_id=\"lysandre/arxiv-nlp\", filename=\"config.json\", revision=\"refs/pr/3\") # Download from a specific commit hash >>> hf_hub_download(repo_id=\"lysandre/arxiv-nlp\", filename=\"config.json\", revision=\"877b84a8f93f2d619faa2a6e514a32beef88ab0a\") Note: When using the commit hash, it must be the full-length hash instead of a 7-character commit hash. Construct a download URL In case you want to construct the URL used to download a file from a repo, you can use hf_hub_url() which returns a URL. Note that it is used internally by hf_hub_download(). Download an entire repository snapshot_download() downloads an entire repository at a given revision. It uses internally hf_hub_download() which means all downloaded files are also cached on your local disk. Downloads', metadata={'source': '../../data/voice_assistant_kb.txt'}), Document(page_content='Hugging Face Models Datasets Spaces Docs Solutions Pricing Log In Sign Up Hub Python Library documentation Upload files to the Hub Hub Python Library Search documentation mainv0.16.3v0.15.1v0.14.1v0.13.4v0.12.1v0.11.0v0.10.1v0.9.1v0.8.1v0.7.0.rc0v0.6.0.rc0v0.5.1 EN Get started Home Quickstart Installation How-to guides Overview Download files Upload files HfFileSystem Repository Search Inference Community Tab Cache Model Cards Manage your Space Integrate a library Webhooks server Conceptual guides Git vs HTTP paradigm Reference Overview Login and logout Environment variables Managing local and online repositories Hugging Face Hub API Downloading files Mixins & serialization methods Inference Client HfFileSystem Utilities Discussions and Pull Requests Cache-system reference Repo Cards and Repo Card Data Space runtime TensorBoard logger Webhooks server Join the Hugging Face community and get access to the augmented documentation experience Collaborate on models, datasets and Spaces Faster examples with accelerated inference Switch between documentation themes Sign Up to get started Upload files to the Hub Sharing your files and work is an important aspect of the Hub. The huggingface_hub offers several options for uploading your files to the Hub. You can use these functions independently or integrate them into your library, making it more convenient for your users to interact with the Hub. This guide will show you how to push files: without using Git. that are very large with Git LFS. with the commit context manager. with the push_to_hub() function. Whenever you want to upload files to the Hub, you need to log in to your Hugging Face account: Log in to your Hugging Face account with the following command: Copied huggingface-cli login # or using an environment variable huggingface-cli login --token $HUGGINGFACE_TOKEN Alternatively, you can programmatically login using login() in a notebook or a script: Copied >>> from huggingface_hub import login >>> login() If ran in a Jupyter or Colaboratory notebook, login() will launch a widget from which you can enter your Hugging Face access token. Otherwise, a message will be prompted in the terminal. It is also possible to login programmatically without the widget by directly passing the token to login(). If you do so, be careful when sharing your notebook. It is best practice to load the token from a secure vault instead of saving it in plain in your Colaboratory notebook. Upload a file Once you’ve created a repository with create_repo(), you can upload a file to your repository using upload_file(). Specify the path of the file to upload, where you want to upload the file to in the repository, and the name of the repository you want to add the file to. Depending on your repository type, you can optionally set the repository type as a dataset, model, or space. Copied >>> from huggingface_hub import HfApi >>> api = HfApi() >>> api.upload_file( ... path_or_fileobj=\"/path/to/local/folder/README.md\", ... path_in_repo=\"README.md\", ... repo_id=\"username/test-dataset\", ... repo_type=\"dataset\", ... ) Upload a folder Use the upload_folder() function to upload a local folder to an existing repository. Specify the path of the local folder to upload, where you want to upload the folder to in the repository, and the name of the repository you want to add the folder to. Depending on your repository type, you can optionally set the repository type as a dataset, model, or space. Copied >>> from huggingface_hub import HfApi >>> api = HfApi() # Upload all the content from the local folder to your remote Space. # By default, files are uploaded at the root of the repo >>> api.upload_folder( ... folder_path=\"/path/to/local/space\", ... repo_id=\"username/my-cool-space\", ... repo_type=\"space\", ... ) Use the allow_patterns and ignore_patterns arguments to specify which files to upload. These parameters accept either a single pattern or a list of patterns. Patterns are Standard Wildcards (globbing patterns) as', metadata={'source': '../../data/voice_assistant_kb.txt'}), Document(page_content=\"Hugging Face Models Datasets Spaces Docs Solutions Pricing Log In Sign Up Hub Python Library documentation How-to guides Hub Python Library Search documentation mainv0.16.3v0.15.1v0.14.1v0.13.4v0.12.1v0.11.0v0.10.1v0.9.1v0.8.1v0.7.0.rc0v0.6.0.rc0v0.5.1 EN Get started Home Quickstart Installation How-to guides Overview Download files Upload files HfFileSystem Repository Search Inference Community Tab Cache Model Cards Manage your Space Integrate a library Webhooks server Conceptual guides Git vs HTTP paradigm Reference Overview Login and logout Environment variables Managing local and online repositories Hugging Face Hub API Downloading files Mixins & serialization methods Inference Client HfFileSystem Utilities Discussions and Pull Requests Cache-system reference Repo Cards and Repo Card Data Space runtime TensorBoard logger Webhooks server Join the Hugging Face community and get access to the augmented documentation experience Collaborate on models, datasets and Spaces Faster examples with accelerated inference Switch between documentation themes Sign Up to get started How-to guides In this section, you will find practical guides to help you achieve a specific goal. Take a look at these guides to learn how to use huggingface_hub to solve real-world problems: Repository How to create a repository on the Hub? How to configure it? How to interact with it? Download files How do I download a file from the Hub? How do I download a repository? Upload files How to upload a file or a folder? How to make changes to an existing repository on the Hub? Search How to efficiently search through the 200k+ public models, datasets and spaces? HfFileSystem How to interact with the Hub through a convenient interface that mimics Python's file interface? Inference How to make predictions using the accelerated Inference API? Community Tab How to interact with the Community tab (Discussions and Pull Requests)? Cache How does the cache-system work? How to benefit from it? Model Cards How to create and share Model Cards? Manage your Space How to manage your Space hardware and configuration? Integrate a library What does it mean to integrate a library with the Hub? And how to do it? Webhooks server How to create a server to receive Webhooks and deploy it as a Space? ←Installation Download files→ How-to guides\", metadata={'source': '../../data/voice_assistant_kb.txt'}), Document(page_content='repository snapshot_download() downloads an entire repository at a given revision. It uses internally hf_hub_download() which means all downloaded files are also cached on your local disk. Downloads are made concurrently to speed-up the process. To download a whole repository, just pass the repo_id and repo_type: Copied >>> from huggingface_hub import snapshot_download >>> snapshot_download(repo_id=\"lysandre/arxiv-nlp\") \\'/home/lysandre/.cache/huggingface/hub/models--lysandre--arxiv-nlp/snapshots/894a9adde21d9a3e3843e6d5aeaaf01875c7fade\\' # Or from a dataset >>> snapshot_download(repo_id=\"google/fleurs\", repo_type=\"dataset\") \\'/home/lysandre/.cache/huggingface/hub/datasets--google--fleurs/snapshots/199e4ae37915137c555b1765c01477c216287d34\\' snapshot_download() downloads the latest revision by default. If you want a specific repository revision, use the revision parameter: Copied >>> from huggingface_hub import snapshot_download >>> snapshot_download(repo_id=\"lysandre/arxiv-nlp\", revision=\"refs/pr/1\") Filter files to download snapshot_download() provides an easy way to download a repository. However, you don’t always want to download the entire content of a repository. For example, you might want to prevent downloading all .bin files if you know you’ll only use the .safetensors weights. You can do that using allow_patterns and ignore_patterns parameters. These parameters accept either a single pattern or a list of patterns. Patterns are Standard Wildcards (globbing patterns) as documented here. The pattern matching is based on fnmatch. For example, you can use allow_patterns to only download JSON configuration files: Copied >>> from huggingface_hub import snapshot_download >>> snapshot_download(repo_id=\"lysandre/arxiv-nlp\", allow_patterns=\"*.json\") On the other hand, ignore_patterns can exclude certain files from being downloaded. The following example ignores the .msgpack and .h5 file extensions: Copied >>> from huggingface_hub import snapshot_download >>> snapshot_download(repo_id=\"lysandre/arxiv-nlp\", ignore_patterns=[\"*.msgpack\", \"*.h5\"]) Finally, you can combine both to precisely filter your download. Here is an example to download all json and markdown files except vocab.json. Copied >>> from huggingface_hub import snapshot_download >>> snapshot_download(repo_id=\"gpt2\", allow_patterns=[\"*.md\", \"*.json\"], ignore_patterns=\"vocab.json\") Download file(s) to local folder The recommended (and default) way to download files from the Hub is to use the cache-system. You can define your cache location by setting cache_dir parameter (both in hf_hub_download() and snapshot_download()). However, in some cases you want to download files and move them to a specific folder. This is useful to get a workflow closer to what git commands offer. You can do that using the local_dir and local_dir_use_symlinks parameters: local_dir must be a path to a folder on your system. The downloaded files will keep the same file structure as in the repo. For example if filename=\"data/train.csv\" and local_dir=\"path/to/folder\", then the returned filepath will be \"path/to/folder/data/train.csv\". local_dir_use_symlinks defines how the file must be saved in your local folder.The default behavior (\"auto\") is to duplicate small files (<5MB) and use symlinks for bigger files. Symlinks allow to optimize both bandwidth and disk usage. However manually editing a symlinked file might corrupt the cache, hence the duplication for small files. The 5MB threshold can be configured with the HF_HUB_LOCAL_DIR_AUTO_SYMLINK_THRESHOLD environment variable. If local_dir_use_symlinks=True is set, all files are symlinked for an optimal disk space optimization. This is for example useful when downloading a huge dataset with thousands of small files. Finally, if you don’t want symlinks at all you can disable them (local_dir_use_symlinks=False). The cache directory will still be used to check wether the file is already cached or not. If already cached, the file is duplicated from the', metadata={'source': '../../data/voice_assistant_kb.txt'})]}\n",
      "\n",
      "Response as speech saved at ../../temp/output.mp3\n"
     ]
    }
   ],
   "source": [
    "main(record_sec=8)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
