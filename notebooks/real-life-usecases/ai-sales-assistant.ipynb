{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI Sales Assistant\n",
    "Develop a sales assistant that will give advice to salesman, taking into considerations internal guidelines, using LangChain.\n",
    "\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook will reveal the challenges faced and the solutions discovered during the creation of this project. You'll learn about the two distinct text-splitting approaches that didn't work and how these failures paved the way for an effective solution.\n",
    "\n",
    "Firstly, the author tried to rely solely on the LLM, but he encountered issues such as response inconsistency and slow response times with GPT-4. Secondly, he naively split the custom knowledge base into chunks, but this approach led to context misalignment and inefficient results.\n",
    "\n",
    "After these unsuccessful attempts, **a more intelligent way of splitting the knowledge base based on its structure was adopted**. This change greatly improved the quality of responses and ensured better context grounding for LLM responses. This process is explained in detail, helping you grasp how to navigate similar challenges in your own AI projects.\n",
    "\n",
    "By the end of this lesson, you'll learn how to utilize LLMs, how to intelligently split your knowledge base, and integrate it with a vector database like Deep Lake for optimal performance."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Workflow\n",
    "This project aims to build a assistant that leverages GPT4 to search for answers within documents. The workflow for the experiment is explained in the following diagram:\n",
    "<br/>\n",
    "<img src=\"../../images/sales-assistant-workflow.png\" alt=\"Sales Assistant Workflow\" style=\"width: 70%; height: auto;\"/>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "_ = load_dotenv(find_dotenv())\n",
    "openai.api_type = os.environ.get(\"OPENAI_API_TYPE\")\n",
    "openai.api_base = os.environ.get(\"OPENAI_API_BASE\")\n",
    "openai.api_version = os.environ.get(\"OPENAI_API_VERSION\")\n",
    "openai.api_key = os.environ.get(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the assistant"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating, Loading, and Querying Our Database for AI\n",
    "For our knowledge base, rather than splitting the text based on size, why don't we split the text based on its structure? We want each chunk to begin with the objection, and end before the \"Objection\" of the next chunk. Based on the data we have, we want the following:\n",
    "\n",
    "If the content string is `1. First sentence. 2. Second sentence.`, the `re.split(r\"(?=\\d+\\. )\", content)` function will split the string into a list as `['1. First sentence. ', '2. Second sentence.']`, effectively separating the sentences based on the numbered format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import DeepLake\n",
    "\n",
    "\n",
    "class DeepLakeLoader:\n",
    "    def __init__(self, source_data_path):\n",
    "        self.source_data_path = source_data_path\n",
    "        self.file_name = os.path.basename(\n",
    "            source_data_path\n",
    "        )  # What we'll name our database\n",
    "        self.data = self.split_data()\n",
    "        if self.check_if_db_exists():\n",
    "            self.db = self.load_db()\n",
    "        else:\n",
    "            self.db = self.create_db()\n",
    "\n",
    "    def check_if_db_exists(self):\n",
    "        \"\"\"\n",
    "        Check if the database already exists.\n",
    "        Returns:\n",
    "            bool: True if the database exists, False otherwise.\n",
    "        \"\"\"\n",
    "        return os.path.exists(f\"deeplake/{self.file_name}\")\n",
    "\n",
    "    def split_data(self):\n",
    "        \"\"\"\n",
    "        Preprocess the data by splitting it into passages.\n",
    "\n",
    "        If using a different data source, this function will need to be modified.\n",
    "\n",
    "        Returns:\n",
    "            split_data (list): List of passages.\n",
    "        \"\"\"\n",
    "        with open(self.source_data_path, \"r\") as f:\n",
    "            content = f.read()\n",
    "        split_data = re.split(r\"(?=\\d+\\. )\", content)\n",
    "        if split_data[0] == \"\":\n",
    "            split_data.pop(0)\n",
    "        split_data = [entry for entry in split_data if len(entry) >= 30]\n",
    "        return split_data\n",
    "\n",
    "    def load_db(self):\n",
    "        \"\"\"\n",
    "        Load the database if it already exists.\n",
    "\n",
    "        Returns:\n",
    "            DeepLake: DeepLake object.\n",
    "        \"\"\"\n",
    "        return DeepLake(\n",
    "            dataset_path=f\"deeplake/{self.file_name}\",\n",
    "            embedding_function=HuggingFaceEmbeddings(),\n",
    "            read_only=True,\n",
    "        )\n",
    "\n",
    "    def create_db(self):\n",
    "        \"\"\"\n",
    "        Create the database if it does not already exist.\n",
    "\n",
    "        Databases are stored in the deeplake directory.\n",
    "\n",
    "        Returns:\n",
    "            DeepLake: DeepLake object.\n",
    "        \"\"\"\n",
    "        return DeepLake.from_texts(\n",
    "            self.data,\n",
    "            HuggingFaceEmbeddings(),\n",
    "            dataset_path=f\"deeplake/{self.file_name}\",\n",
    "        )\n",
    "\n",
    "    def query_db(self, query):\n",
    "        \"\"\"\n",
    "        Query the database for passages that are similar to the query.\n",
    "\n",
    "        Args:\n",
    "            query (str): Query string.\n",
    "\n",
    "        Returns:\n",
    "            content (list): List of passages that are similar to the query.\n",
    "        \"\"\"\n",
    "        results = self.db.similarity_search(query, k=3)\n",
    "        content = []\n",
    "        for result in results:\n",
    "            content.append(result.page_content)\n",
    "        return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset(path='deeplake/salestesting.txt', tensors=['embedding', 'id', 'metadata', 'text'])\n",
      "\n",
      "  tensor      htype      shape     dtype  compression\n",
      "  -------    -------    -------   -------  ------- \n",
      " embedding  embedding  (44, 768)  float32   None   \n",
      "    id        text      (44, 1)     str     None   \n",
      " metadata     json      (44, 1)     str     None   \n",
      "   text       text      (44, 1)     str     None   \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "db = DeepLakeLoader(\"../../data/salestesting.txt\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to query the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chat_models import AzureChatOpenAI\n",
    "\n",
    "\n",
    "def query_model(query: str) -> str:\n",
    "    # retrieve relevant chunks\n",
    "    retrieved_chunks = db.query_db(query)\n",
    "\n",
    "    # prompt template\n",
    "    template = \"\"\"\n",
    "    You are SalesCopilot. You will be provided with a customer objection, and a selection \\\n",
    "    of guidelines on how to respond to certain objections. \\\n",
    "    Using the provided content, write out the objection and the actionable course of action you recommend \\\n",
    "    in the following format:\n",
    "    {delimeter}\n",
    "    It seems like the customer is <explain their objection>.\n",
    "    I recommend you <course of action>.\n",
    "    {delimeter}\n",
    "\n",
    "    Objection: {delimeter} {objection} {delimeter}\n",
    "    Guidelines: {delimeter} {guidelines} {delimeter}\n",
    "    \"\"\"\n",
    "\n",
    "    prompt = PromptTemplate(\n",
    "        input_variables=[\"objection\", \"guidelines\", \"delimeter\"], template=template\n",
    "    )\n",
    "\n",
    "    # format the prompt\n",
    "    prompt_formatted = prompt.format(\n",
    "        objection=query, guidelines=retrieved_chunks, delimeter=\"----\"\n",
    "    )\n",
    "\n",
    "    # generate answer\n",
    "    llm = AzureChatOpenAI(deployment_name=\"gpt4\", temperature=0)\n",
    "    answer = llm.predict(prompt_formatted)\n",
    "    return answer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Call the function with user query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It seems like the customer is planning to use their budget on something more important.\n",
      "I recommend you share case studies of similar companies that have saved money, increased efficiency, or had a massive ROI with your product/service, making it a priority that deserves budget allocation now. You can say, \"We had a customer with a similar issue, but by purchasing [product] they were actually able to increase their ROI and assign some of their new revenue to other parts of the budget.\"\n"
     ]
    }
   ],
   "source": [
    "# user question\n",
    "query = \"we have planned to use our budget on something more important\"\n",
    "\n",
    "answer = query_model(query)\n",
    "print(answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
