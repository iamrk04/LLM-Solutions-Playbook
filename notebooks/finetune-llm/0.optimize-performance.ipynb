{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimize LLM Performance\n",
    "Learn how to optimize the performance of the LLM model, and when to use which technique.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to Optimize LLM Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../../images/llm-optimization.png\" alt=\"LLM Optimization\" style=\"width: 70%; height: auto;\"/>\n",
    "<br/>\n",
    "\n",
    "1. Always start with prompt engineeering. **Will lead to more token usage**.\n",
    "2. If you want to add more knowledge to the model, use RAG. **Will lead to even more token usage**.\n",
    "3. If you want to change the format or style or tone in which output is being generated while keeping the token usage in check, use finetuning. **Finetuning doesn't add more knowledge to the model**.\n",
    "4. Finally, you can use all of the techniques combined to get the best of all worlds.\n",
    "\n",
    "| Technique | Good for | Not good for |\n",
    "| --- | --- | --- |\n",
    "| **Prompt Engineering** | <li> Testing and learning early. <li> Quick to implement. <li> Change the structure or tone of the output. | <li> Introducing new information. <li> Minimizing token usage. |\n",
    "| **RAG** | <li> Introducing new information to the model. <li> Reducing hallucinations by controlling content. | <li> Embedding understanding of a broad domain. <li> Teaching the model to learn a new language, format or style. <li> Reducing token usage |\n",
    "| **Finetuning** | <li> Emphasizing knowledge that is already present in the model. <li> Changing the format or style or tone in which output is being generated. <li> Teaching model very complex instruction. <li> Reducing token usage. | <li> Adding more knowledge to the model. <li> Quickly iterating in a new use-case.|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLMs Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLM Scaling Laws"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scaling laws refer to the relationship between the model's performance and factors such as the number of parameters, the size of the training dataset, the compute budget, and the network architecture.\n",
    "\n",
    "The main elements characterizing a language model are:\n",
    "1. The number of parameters (N) reflects the model's capacity to learn from data. More parameters allow the model to capture complex patterns in the data.\n",
    "2. The size of the training dataset (D) is measured in the number of tokens (small pieces of text ranging from a few words to a single character).\n",
    "3. FLOPs (floating point operations per second) measure the compute budget used for training.\n",
    "\n",
    "> For a model with X parameters, it is optimal to train it on approximately X * 20 tokens. For example, a model with 100 billion parameters would be optimally trained on approximately 2 trillion tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Emergent Abilities in LLMs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Emergent abilities in LLMs refer to the sudden appearance of new capabilities as the size of the model increases. These abilities, which include performing arithmetic, answering questions, summarizing passages, and more, are not explicitly trained in the model. Instead, they seem to arise spontaneously as the model scales, hence the term \"emergent.\"\n",
    "\n",
    "As LLMs grow, they rapidly and unpredictably transition from near-zero to sometimes state-of-the-art performance. This phenomenon suggests that these abilities are emergent properties of the model's scale rather than being explicitly programmed into the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mitgate Hallucinations and Bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Use good data for pre-training as well as fine-tuning.\n",
    "2. Tweak inference parameters:\n",
    "    - **temperature**: higher value leads to randomness and creativity; lower value leads to deterministic output.\n",
    "    - **frequency penalty**: higher value leads to less repetition of tokens (an already present token will be repeated very less).\n",
    "    - **presence penalty**: higher value promotes including tokens that haven't been generated yet, rare tokens can be generated (more strict, an already present token won't be generated); lower value leads to generation of common tokens. \n",
    "    - **top-p**: controls response diversity by sampling from the smallest possible set of tokens whose cumulative probability exceeds the probability p.\n",
    "3. Prompt Engineering\n",
    "4. RAG\n",
    "5. Finetuning on high-quality unbiased data\n",
    "6. Constitutional AI (RLAIF)\n",
    "7. Stay upto date with the latest research as this is still an active area of research."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Training Process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Pretraining LLMs**: Models are trained on massive text corpora sourced from the Internet, honing their linguistic knowledge through the prediction of the following words within sentences.\n",
    "2. **Finetuning LLMs**: While pretrained LLMs are undeniably impressive, their true potential is unlocked through finetuning. For example, if the task is to answer questions about medical texts, the model would be finetuned on a dataset of medical question-answer pairs. Finetuning helps these models become specialized. Finetuning exposes pretrained models to task-specific datasets, enabling them to recalibrate internal parameters and representations to align with the intended task. This adaptation enhances their ability to handle domain-specific challenges effectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Instruction Finetuning: Making General-Purpose Assistants out of LLMs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instruction finetuning adds precise control over model behavior, making it a general-purpose assistant. The goal of instruction finetuning is to obtain an LLM that interprets prompts as instructions instead of text. Itâ€™s just a special type of finetuning.\n",
    "\n",
    "For example, consider the following prompt: **What is the capital of France?**\n",
    "- An LLM with instruction finetuning would likely interpret the prompt as an instruction, giving the following answer: **Paris.**\n",
    "- However, a plain LLM without instruction finetuning could think that we are writing a list of exercises for our geography students, therefore merely continuing the text with a new question: **What is the capital of Italy?**\n",
    "\n",
    "Instruction tuning offers several advantages. It trains models on a collection of tasks described via instructions, granting LLMs the capacity to **generalize to new tasks** prompted by additional instructions. This sidesteps the need for vast amounts of task-specific data and instead uses textual instructions to guide learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finetuning Techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are multiple methods in fine-tuning with a focus on the number of parameters, such as:\n",
    "- **Full finetuning**: This method is based on adjusting all the parameters in the pretrained LLM models in order to adapt to a specific task. However, this method is relatively resource-intensive, requiring extensive computational power.\n",
    "- **Low-Rank Adaptation (LoRA)**: LoRA aims to adapt LLMs to specific tasks and datasets while simultaneously reducing computational resources and costs. By applying low-rank approximations to the downstream layers of LLMs, LoRA significantly reduces the number of parameters to be trained, thereby lowering the GPU memory requirements and training costs.\n",
    "\n",
    "Multiple methods are focusing on the learning algorithm used for finetuning, such as:\n",
    "- **Supervised Finetuning (SFT)**: SFT involves doing standard supervised finetuning with a pretrained LLM on a small amount of demonstration data.\n",
    "- **Reinforcement Learning from Human Feedback (RLHF)**: RLHF is a training methodology where models are trained to follow human feedback over multiple iterations."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
