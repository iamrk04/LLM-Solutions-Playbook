{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rules Based Evals\n",
    "\n",
    "Use simple rules to evaluate the LLM's response.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pros:\n",
    "- Quick to implement\n",
    "- Cheap to run\n",
    "- Fast to run\n",
    "\n",
    "Cons:\n",
    "- Can't handle complex rules\n",
    "- Naive way of evaluating\n",
    "\n",
    "**Suitable for CI pipelines to quickly check for any regression without incurring too much cost.**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "_ = load_dotenv(find_dotenv())\n",
    "openai.api_type = os.environ.get(\"OPENAI_API_TYPE\")\n",
    "openai.api_base = os.environ.get(\"OPENAI_API_BASE\")\n",
    "openai.api_key = os.environ.get(\"OPENAI_API_KEY\")\n",
    "openai.api_version = os.environ.get(\"OPENAI_API_VERSION\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import AzureChatOpenAI\n",
    "\n",
    "llm = AzureChatOpenAI(\n",
    "    deployment_name=\"gpt40125\",\n",
    "    temperature=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample app: AI-powered quiz generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to build a AI powered quiz generator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "quiz_bank = \"\"\"\n",
    "1. Subject: Leonardo DaVinci\n",
    "   Categories: Art, Science\n",
    "   Facts:\n",
    "    - Painted the Mona Lisa\n",
    "    - Studied zoology, anatomy, geology, optics\n",
    "    - Designed a flying machine\n",
    "  \n",
    "2. Subject: Paris\n",
    "   Categories: Art, Geography\n",
    "   Facts:\n",
    "    - Location of the Louvre, the museum where the Mona Lisa is displayed\n",
    "    - Capital of France\n",
    "    - Most populous city in France\n",
    "    - Where Radium and Polonium were discovered by scientists Marie and Pierre Curie\n",
    "\n",
    "3. Subject: Telescopes\n",
    "   Category: Science\n",
    "   Facts:\n",
    "    - Device to observe different objects\n",
    "    - The first refracting telescopes were invented in the Netherlands in the 17th Century\n",
    "    - The James Webb space telescope is the largest telescope in space. It uses a gold-berillyum mirror\n",
    "\n",
    "4. Subject: Starry Night\n",
    "   Category: Art\n",
    "   Facts:\n",
    "    - Painted by Vincent van Gogh in 1889\n",
    "    - Captures the east-facing view of van Gogh's room in Saint-RÃ©my-de-Provence\n",
    "\n",
    "5. Subject: Physics\n",
    "   Category: Science\n",
    "   Facts:\n",
    "    - The sun doesn't change color during sunset.\n",
    "    - Water slows the speed of light\n",
    "    - The Eiffel Tower in Paris is taller in the summer than the winter due to expansion of the metal.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "delimiter = \"####\"\n",
    "\n",
    "system_message = f\"\"\"\n",
    "Follow these steps to generate a customized quiz for the user.\n",
    "The question will be delimited with four hashtags i.e {delimiter}\n",
    "\n",
    "The user will provide a category that they want to create a quiz for. Any questions included in the quiz\n",
    "should only refer to the category.\n",
    "\n",
    "Step 1:{delimiter} First identify the category user is asking about from the following list:\n",
    "* Geography\n",
    "* Science\n",
    "* Art\n",
    "\n",
    "Step 2:{delimiter} Determine the subjects to generate questions about. The list of topics are below:\n",
    "\n",
    "{quiz_bank}\n",
    "\n",
    "Pick up to two subjects that fit the user's category. \n",
    "\n",
    "Step 3:{delimiter} Generate a quiz for the user. Based on the selected subjects generate 3 questions for the user using the facts about the subject.\n",
    "\n",
    "Use the following format for the quiz:\n",
    "Question 1:{delimiter} <question 1>\n",
    "\n",
    "Question 2:{delimiter} <question 2>\n",
    "\n",
    "Question 3:{delimiter} <question 3>\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema.output_parser import StrOutputParser\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "\n",
    "def assistant_chain(\n",
    "    system_message,\n",
    "    user_message,\n",
    "    llm=llm,\n",
    "    output_parser=StrOutputParser()\n",
    "):\n",
    "  \n",
    "  chat_prompt = ChatPromptTemplate.from_messages([\n",
    "      (\"system\", system_message),\n",
    "      (\"human\", user_message),\n",
    "  ])\n",
    "  return chat_prompt | llm | output_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'#### Science\\n\\n#### Leonardo DaVinci, Telescopes, Physics\\n\\n#### Question 1:#### What did Leonardo DaVinci design that was centuries ahead of its time?\\nA) A submarine\\nB) A flying machine\\nC) A digital computer\\nD) A solar-powered car\\n\\n#### Question 2:#### Which telescope is known as the largest telescope in space and uses a gold-beryllium mirror?\\nA) Hubble Space Telescope\\nB) Spitzer Space Telescope\\nC) James Webb Space Telescope\\nD) Kepler Space Telescope\\n\\n#### Question 3:#### Which of the following statements about the Eiffel Tower is true?\\nA) It moves closer to the sun in the summer\\nB) It is taller in the summer than in the winter\\nC) It changes color depending on the season\\nD) It was originally designed as a giant telescope'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "assistant = assistant_chain(\n",
    "    system_message=system_message,\n",
    "    user_message=\"Generate a quiz about science.\"\n",
    ")\n",
    "assistant.invoke({})\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test if the generated questions have expected word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_expected_words(\n",
    "    system_message,\n",
    "    user_message,\n",
    "    expected_words,\n",
    "    llm=llm,\n",
    "    output_parser=StrOutputParser()\n",
    "):\n",
    "    \n",
    "    assistant = assistant_chain(\n",
    "        system_message=system_message,\n",
    "        user_message=user_message,\n",
    "        llm=llm,\n",
    "        output_parser=output_parser\n",
    "    )\n",
    "\n",
    "    answer = assistant.invoke({})\n",
    "    print(answer)\n",
    "\n",
    "    assert any(word in answer.lower() for word in expected_words), (\n",
    "        f\"Expected the assistant questions to include '{expected_words}', but it did not.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_message  = \"Generate a quiz about science.\"\n",
    "expected_words = [\"davinci\", \"telescope\", \"physics\", \"curie\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#### Science\n",
      "\n",
      "#### Telescopes, Physics\n",
      "\n",
      "#### Question 1:#### What material is used in the mirror of the James Webb space telescope?\n",
      "- A) Silver\n",
      "- B) Gold-Beryllium\n",
      "- C) Aluminum\n",
      "- D) Titanium\n",
      "\n",
      "#### Question 2:#### Which of the following statements about light is true?\n",
      "- A) The sun changes color during sunset.\n",
      "- B) Water increases the speed of light.\n",
      "- C) The Eiffel Tower is shorter in the summer due to the contraction of metal.\n",
      "- D) Water slows the speed of light.\n",
      "\n",
      "#### Question 3:#### When were the first refracting telescopes invented?\n",
      "- A) 16th Century\n",
      "- B) 17th Century\n",
      "- C) 18th Century\n",
      "- D) 19th Century\n"
     ]
    }
   ],
   "source": [
    "eval_expected_words(\n",
    "    system_message=system_message,\n",
    "    user_message=user_message,\n",
    "    expected_words=expected_words\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test if the app declines to generate questions when category is not in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_refusal(\n",
    "    system_message,\n",
    "    user_message,\n",
    "    decline_response,\n",
    "    llm=llm,\n",
    "    output_parser=StrOutputParser()\n",
    "):\n",
    "    \n",
    "    assistant = assistant_chain(\n",
    "        system_message=system_message,\n",
    "        user_message=user_message,\n",
    "        llm=llm,\n",
    "        output_parser=output_parser\n",
    "    )\n",
    "  \n",
    "    answer = assistant.invoke({})\n",
    "    print(answer)\n",
    "  \n",
    "    assert decline_response.lower() in answer.lower(), (\n",
    "        f\"Expected the bot to decline with '{decline_response}' got {answer}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_message  = \"Generate a quiz about Biology.\"\n",
    "decline_response = \"I am sorry\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Since Biology isn't directly listed in the provided categories or subjects, I'll adapt the closest relevant subject from the Science category to fit a Biology-themed quiz. The subject of Leonardo DaVinci includes studies in zoology and anatomy, which are relevant to Biology. \n",
      "\n",
      "#### Generate a quiz for the user. Based on the selected subject generate 3 questions for the user using the facts about the subject.\n",
      "\n",
      "Question 1:#### What areas of biology did Leonardo DaVinci study?\n",
      "a) Botany and microbiology\n",
      "b) Zoology and anatomy\n",
      "c) Genetics and evolution\n",
      "\n",
      "Question 2:#### Leonardo DaVinci is known for his contributions to art and science. Which of the following did he design that shows his understanding of biology and physics?\n",
      "a) A flying machine\n",
      "b) A submarine\n",
      "c) A telescope\n",
      "\n",
      "Question 3:#### In addition to his biological studies, Leonardo DaVinci painted a famous artwork. What is the name of this painting?\n",
      "a) The Starry Night\n",
      "b) The Mona Lisa\n",
      "c) The Last Supper\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Expected the bot to decline with 'I am sorry' got Since Biology isn't directly listed in the provided categories or subjects, I'll adapt the closest relevant subject from the Science category to fit a Biology-themed quiz. The subject of Leonardo DaVinci includes studies in zoology and anatomy, which are relevant to Biology. \n\n#### Generate a quiz for the user. Based on the selected subject generate 3 questions for the user using the facts about the subject.\n\nQuestion 1:#### What areas of biology did Leonardo DaVinci study?\na) Botany and microbiology\nb) Zoology and anatomy\nc) Genetics and evolution\n\nQuestion 2:#### Leonardo DaVinci is known for his contributions to art and science. Which of the following did he design that shows his understanding of biology and physics?\na) A flying machine\nb) A submarine\nc) A telescope\n\nQuestion 3:#### In addition to his biological studies, Leonardo DaVinci painted a famous artwork. What is the name of this painting?\na) The Starry Night\nb) The Mona Lisa\nc) The Last Supper",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[53], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mevaluate_refusal\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43msystem_message\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msystem_message\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43muser_message\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_message\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecline_response\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecline_response\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[20], line 19\u001b[0m, in \u001b[0;36mevaluate_refusal\u001b[1;34m(system_message, user_message, decline_response, llm, output_parser)\u001b[0m\n\u001b[0;32m     16\u001b[0m answer \u001b[38;5;241m=\u001b[39m assistant\u001b[38;5;241m.\u001b[39minvoke({})\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(answer)\n\u001b[1;32m---> 19\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m decline_response\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;129;01min\u001b[39;00m answer\u001b[38;5;241m.\u001b[39mlower(), (\n\u001b[0;32m     20\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected the bot to decline with \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdecline_response\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00manswer\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     21\u001b[0m )\n",
      "\u001b[1;31mAssertionError\u001b[0m: Expected the bot to decline with 'I am sorry' got Since Biology isn't directly listed in the provided categories or subjects, I'll adapt the closest relevant subject from the Science category to fit a Biology-themed quiz. The subject of Leonardo DaVinci includes studies in zoology and anatomy, which are relevant to Biology. \n\n#### Generate a quiz for the user. Based on the selected subject generate 3 questions for the user using the facts about the subject.\n\nQuestion 1:#### What areas of biology did Leonardo DaVinci study?\na) Botany and microbiology\nb) Zoology and anatomy\nc) Genetics and evolution\n\nQuestion 2:#### Leonardo DaVinci is known for his contributions to art and science. Which of the following did he design that shows his understanding of biology and physics?\na) A flying machine\nb) A submarine\nc) A telescope\n\nQuestion 3:#### In addition to his biological studies, Leonardo DaVinci painted a famous artwork. What is the name of this painting?\na) The Starry Night\nb) The Mona Lisa\nc) The Last Supper"
     ]
    }
   ],
   "source": [
    "evaluate_refusal(\n",
    "    system_message=system_message,\n",
    "    user_message=user_message,\n",
    "    decline_response=decline_response\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Update system message to accomodate the failing test case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message += \"\"\"\n",
    "Do not generate quiz if the category is not in the dataset. For such scenarios, only output \\\n",
    "`I am sorry, this category isn't available in the dataset`.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am sorry, this category isn't available in the dataset.\n"
     ]
    }
   ],
   "source": [
    "evaluate_refusal(\n",
    "    system_message=system_message,\n",
    "    user_message=user_message,\n",
    "    decline_response=decline_response\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
