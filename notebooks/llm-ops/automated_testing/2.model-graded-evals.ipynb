{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Graded Evals\n",
    "\n",
    "Use an LLM to evaluate another LLM's response.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pros:\n",
    "- More robust and compehensive\n",
    "\n",
    "Cons:\n",
    "- Costly to run\n",
    "- Slower to run\n",
    "\n",
    "**Suitable for pre-deployment and post-deployment to guarantee full confidence in our LLM app.**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "_ = load_dotenv(find_dotenv())\n",
    "openai.api_type = os.environ.get(\"OPENAI_API_TYPE\")\n",
    "openai.api_base = os.environ.get(\"OPENAI_API_BASE\")\n",
    "openai.api_key = os.environ.get(\"OPENAI_API_KEY\")\n",
    "openai.api_version = os.environ.get(\"OPENAI_API_VERSION\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import AzureChatOpenAI\n",
    "\n",
    "llm = AzureChatOpenAI(\n",
    "    deployment_name=\"gpt40125\",\n",
    "    temperature=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample app: AI-powered quiz generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test if the generated quiz is in the right fomat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "delimiter = \"####\"\n",
    "\n",
    "eval_system_message = f\"\"\"You are an assistant that evaluates \\\n",
    "whether or not an assistant is producing valid quizzes.\n",
    "The assistant should be producing output in the \\\n",
    "format of Question N:{delimiter} <question N>?\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_llm_response = \"\"\"\n",
    "Question 1:#### What is the largest telescope in space called and what material is its mirror made of?\n",
    "\n",
    "Question 2:#### True or False: Water slows down the speed of light.\n",
    "\n",
    "Question 3:#### What did Marie and Pierre Curie discover in Paris?\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_user_message_template = \"\"\"You are evaluating a generated quiz \\\n",
    "based on the context that the assistant uses to create the quiz.\n",
    "  Here is the data:\n",
    "    [BEGIN DATA]\n",
    "    ************\n",
    "    [Response]: {llm_response}\n",
    "    ************\n",
    "    [END DATA]\n",
    "\n",
    "Read the response carefully and determine if it looks like \\\n",
    "a quiz or not. Do not evaluate if the information is correct\n",
    "only evaluate if the data is in the expected format.\n",
    "\n",
    "Output Y if the response is a quiz, \\\n",
    "output N if the response does not look like a quiz.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema.output_parser import StrOutputParser\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "\n",
    "def assistant_chain(\n",
    "    system_message,\n",
    "    user_message_template,\n",
    "    llm_response,\n",
    "    llm=llm,\n",
    "    output_parser=StrOutputParser()\n",
    "):\n",
    "    user_message = user_message_template.format(llm_response=llm_response)\n",
    "    chat_prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", system_message),\n",
    "        (\"human\", user_message),\n",
    "    ])\n",
    "    return chat_prompt | llm | output_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Y'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "assistant = assistant_chain(\n",
    "    system_message=eval_system_message,\n",
    "    user_message_template=eval_user_message_template,\n",
    "    llm_response=sample_llm_response,\n",
    ")\n",
    "assistant.invoke({})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test for negative test case when the LLM response is not in the right format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_llm_response = \"There are lots of interesting facts. Tell me more about what you'd like to know.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'N'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "assistant = assistant_chain(\n",
    "    system_message=eval_system_message,\n",
    "    user_message_template=eval_user_message_template,\n",
    "    llm_response=bad_llm_response,\n",
    ")\n",
    "assistant.invoke({})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
