{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models Detailed\n",
    "- [LLMs vs Chat Models](#llms-vs-chat-models)\n",
    "- [Open-Source GPT4All](#open-source-gpt4all)\n",
    "- [HuggingFace LLM](#huggingface-llm)\n",
    "\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLMs vs Chat Models"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLMs\n",
    "LLMs, such as GPT-3, Bloom, PaLM, and Aurora genAI, take a text string as input and return a text string as output. They are trained on language modeling tasks and can generate human-like text, perform complex reasoning, and even write code. LLMs are powerful and flexible, capable of generating text for a wide range of tasks. However, they can sometimes produce incorrect or nonsensical answers, and their API is less structured compared to Chat Models. You can use classes from `langchain.llms` to interact with LLMs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "llm = OpenAI(model_name=\"text-davinci-003\", temperature=0)\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"product\"],\n",
    "    template=\"What is a good name for a company that makes {product}?\",\n",
    ")\n",
    "\n",
    "chain = LLMChain(llm=llm, prompt=prompt)\n",
    "\n",
    "print(chain.run(\"wireless headphones\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chat Models\n",
    "Chat Models are the most popular models in LangChain, such as ChatGPT that can incorporate GPT-3 or GPT-4 at its core. They have gained significant attention due to their ability to learn from human feedback and their user-friendly chat interface.\n",
    "\n",
    "Chat Models, such as ChatGPT, take a list of messages as input and return an `AIMessage`. They typically use LLMs as their underlying technology, but their APIs are more structured. Chat Models are designed to remember previous exchanges with the user in a session and use that context to generate more relevant responses. They also benefit from reinforcement learning from human feedback, which helps improve their responses. You can use classes from `langchain.chat_models` to interact with Chat Models.\n",
    "\n",
    "Chat Message Types: `SystemMessage`, `HumanMessage` and `AIMessage`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"J'aime la programmation.\", additional_kwargs={}, example=False)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chat_models import AzureChatOpenAI\n",
    "from langchain.schema import HumanMessage, SystemMessage\n",
    "\n",
    "chat = AzureChatOpenAI(deployment_name=\"gpt4\", temperature=0)\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(\n",
    "        content=\"You are a helpful assistant that translates English to French.\"\n",
    "    ),\n",
    "    HumanMessage(content=\"Translate the following sentence: I love programming.\"),\n",
    "]\n",
    "\n",
    "chat(messages)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the `generate()` method, you can also generate completions for multiple sets of messages. Each batch of messages can have its own `SystemMessage` and will perform independently. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generations=[[ChatGeneration(text=\"J'aime la programmation.\", generation_info=None, message=AIMessage(content=\"J'aime la programmation.\", additional_kwargs={}, example=False))], [ChatGeneration(text='I love programming.', generation_info=None, message=AIMessage(content='I love programming.', additional_kwargs={}, example=False))]] llm_output={'token_usage': {'completion_tokens': 11, 'prompt_tokens': 65, 'total_tokens': 76}, 'model_name': 'gpt-3.5-turbo'} run=[RunInfo(run_id=UUID('0ec7f864-7474-4736-9009-7006d8712540')), RunInfo(run_id=UUID('e473b13b-84db-4486-86be-455800947fff'))]\n"
     ]
    }
   ],
   "source": [
    "batch_messages = [\n",
    "    [\n",
    "        SystemMessage(\n",
    "            content=\"You are a helpful assistant that translates English to French.\"\n",
    "        ),\n",
    "        HumanMessage(content=\"Translate the following sentence: I love programming.\"),\n",
    "    ],\n",
    "    [\n",
    "        SystemMessage(\n",
    "            content=\"You are a helpful assistant that translates French to English.\"\n",
    "        ),\n",
    "        HumanMessage(\n",
    "            content=\"Translate the following sentence: J'aime la programmation.\"\n",
    "        ),\n",
    "    ],\n",
    "]\n",
    "print(chat.generate(batch_messages))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Open-Source GPT4All"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The GPT-family models are undoubtedly powerful. However, access to these models' weights and architecture is restricted, and even if one does have access, it requires significant resources to perform any task.\n",
    "\n",
    "Furthermore, the available APIs are not free to build on top of. These limitations can restrict the ongoing research on Large Language Models (LLMs). The alternative open-source models (like GPT4All) aim to overcome these obstacles and make the LLMs more accessible to everyone.\n",
    "\n",
    "The main contribution of GPT4All models is the ability to run them on a CPU. Testing these models is practically free because the recent PCs have powerful Central Processing Units. It is true that we are sacrificing quality by a small margin when using this approach. However, it is a trade-off between no access at all and accessing a slightly underpowered model!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert the Model\n",
    "The first step is to download the weights and use a script from the LLaMAcpp repository to convert the weights from the old format to the new one. It is a required step; otherwise, the LangChain library will not identify the checkpoint file.\n",
    "\n",
    "> Note: The cell below will take a while since the file size is 4GB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "385639it [07:02, 912.31it/s] \n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "local_path = \"../../models/gpt4all-lora-quantized-ggml.bin\"\n",
    "Path(local_path).parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "url = \"https://the-eye.eu/public/AI/models/nomic-ai/gpt4all/gpt4all-lora-quantized-ggml.bin\"\n",
    "\n",
    "# send a GET request to the URL to download the file.\n",
    "response = requests.get(url, stream=True)\n",
    "\n",
    "# open the file in binary mode and write the contents of the response\n",
    "# to it in chunks.\n",
    "with open(local_path, \"wb\") as f:\n",
    "    for chunk in tqdm(response.iter_content(chunk_size=8192)):\n",
    "        if chunk:\n",
    "            f.write(chunk)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, it is time to transform the downloaded file to the latest format. We start by downloading the codes in the LLaMAcpp repository or simply fork it using the following command. The script will create a new file in the models directory with the following name `ggml-model-q4_0.bin`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "commands = [\n",
    "    \"git clone https://github.com/ggerganov/llama.cpp.git\",\n",
    "    \"python ./llama.cpp/convert.py ../../models/gpt4all-lora-quantized-ggml.bin\",\n",
    "]\n",
    "for command in commands:\n",
    "    result = subprocess.run(command, shell=True, capture_output=True, text=True)\n",
    "    if result.returncode == 0:\n",
    "        print(\"Command executed successfully!\")\n",
    "        # print(\"Output:\")\n",
    "        # print(result.stdout)\n",
    "    else:\n",
    "        print(\"Command execution failed!\")\n",
    "        print(\"Error message:\")\n",
    "        print(result.stderr)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Model and Generate\n",
    "The default behavior is to wait for the model to finish its inference process to print out its outputs. However, it could take more than an hour (depending on your hardware) to respond to one prompt because of the large number of parameters in the model. We can use the `StreamingStdOutCallbackHandler()` callback to instantly show the latest generated token. This way, we can be sure that the generation process is running and the model shows the expected behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import GPT4All\n",
    "from langchain import PromptTemplate, LLMChain\n",
    "from langchain.callbacks.manager import CallbackManager\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found model file at  ../../models/ggml-model-q4_0.bin\n"
     ]
    }
   ],
   "source": [
    "callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])\n",
    "llm = GPT4All(\n",
    "    model=\"../../models/ggml-model-q4_0.bin\",\n",
    "    callback_manager=callback_manager,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"Question: {question}\n",
    "\n",
    "Answer: Let's think step by step.\"\"\"\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"question\"])\n",
    "llm_chain = LLMChain(prompt=prompt, llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " When rain falls, the water droplets fall from clouds in the sky and hit different surfaces on Earth such as roads or trees. The amount of precipitation that can occur varies depending upon many factors including air temperature and humidity levels surrounding a particular area where it is raining. During heavy downpour conditions, rainwater accumulates at lower elevations forming puddles which ultimately lead to surface runoff as the water flows through gutters or other drains on its way towards nearby rivers/streams etc..\n",
      " When rain falls, the water droplets fall from clouds in the sky and hit different surfaces on Earth such as roads or trees. The amount of precipitation that can occur varies depending upon many factors including air temperature and humidity levels surrounding a particular area where it is raining. During heavy downpour conditions, rainwater accumulates at lower elevations forming puddles which ultimately lead to surface runoff as the water flows through gutters or other drains on its way towards nearby rivers/streams etc.."
     ]
    },
    {
     "data": {
      "text/plain": [
       "' When rain falls, the water droplets fall from clouds in the sky and hit different surfaces on Earth such as roads or trees. The amount of precipitation that can occur varies depending upon many factors including air temperature and humidity levels surrounding a particular area where it is raining. During heavy downpour conditions, rainwater accumulates at lower elevations forming puddles which ultimately lead to surface runoff as the water flows through gutters or other drains on its way towards nearby rivers/streams etc..'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"What happens when it rains somewhere?\"\n",
    "llm_chain.run(question)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another prompt for the same question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"Question: {question}\n",
    "\n",
    "Answer: Let's answer in two sentence while being funny.\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"question\"])\n",
    "llm_chain = LLMChain(prompt=prompt, llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " When rain falls, some places turn into swimming pools and others become river beds as they collect the precipitation until a flood occurs which makes people scream for help due to rising water levels around them!\n",
      " When rain falls, some places turn into swimming pools and others become river beds as they collect the precipitation until a flood occurs which makes people scream for help due to rising water levels around them!"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' When rain falls, some places turn into swimming pools and others become river beds as they collect the precipitation until a flood occurs which makes people scream for help due to rising water levels around them!'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"What happens when it rains somewhere?\"\n",
    "llm_chain.run(question)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Llama 2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download any Llama model that you are interested in from [here](https://huggingface.co/TheBloke). For this example, we will be using the `llama-2-13b-chat.ggmlv3.q4_1.bin` from [here](https://huggingface.co/TheBloke/Llama-2-13B-chat-GGML/tree/main)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | VSX = 0 | \n"
     ]
    }
   ],
   "source": [
    "from langchain.llms import LlamaCpp\n",
    "from langchain.callbacks.manager import CallbackManager\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain import PromptTemplate, LLMChain\n",
    "\n",
    "\n",
    "callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])\n",
    "llm = LlamaCpp(\n",
    "    model_path=\"../../models/llama-2-13b-chat.ggmlv3.q4_1.bin\",\n",
    "    temperature=0.8,\n",
    "    # n_threads=8,\n",
    "    # n_ctx=2048,\n",
    "    # n_batch=256,\n",
    "    callback_manager=callback_manager,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"SYSTEM: You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe. \\\n",
    "Answer has to be as short as possible without losing the meaning. If you don't know the answer to a question, please don't share false information.\n",
    "USER: {question}\n",
    "ASSISTANT: \n",
    "\"\"\"\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"question\"])\n",
    "llm_chain = LLMChain(prompt=prompt, llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Good day! The capital of India is New Delhi.\n",
      "Reponse from model: Good day! The capital of India is New Delhi.\n"
     ]
    }
   ],
   "source": [
    "question = \"What is the capital of India?\"\n",
    "response = llm_chain.run(question)\n",
    "print(f\"\\nReponse from model: {response}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HuggingFace LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate\n",
    "\n",
    "template = \"\"\"Question: {question}\n",
    "\n",
    "Answer: \"\"\"\n",
    "\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"question\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "paris\n"
     ]
    }
   ],
   "source": [
    "from langchain import HuggingFaceHub, LLMChain\n",
    "\n",
    "# initialize Hub LLM\n",
    "hub_llm = HuggingFaceHub(\n",
    "    repo_id=\"google/flan-t5-large\", model_kwargs={\"temperature\": 0, \"max_length\": 128}\n",
    ")\n",
    "\n",
    "# create prompt template > LLM chain\n",
    "llm_chain = LLMChain(prompt=prompt, llm=hub_llm)\n",
    "\n",
    "# user question\n",
    "question = \"What is the capital city of France?\"\n",
    "\n",
    "# ask the user question about the capital of France\n",
    "print(llm_chain.run(question))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Asking multiple questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generations=[[Generation(text='paris', generation_info=None)], [Generation(text='giraffe', generation_info=None)], [Generation(text='nitrogen', generation_info=None)], [Generation(text='yellow', generation_info=None)]] llm_output=None run=[RunInfo(run_id=UUID('c4c1c931-b9a9-491e-a978-af64deea8780')), RunInfo(run_id=UUID('0f0ea5d1-7642-4e7f-9904-9efb199db309')), RunInfo(run_id=UUID('52a209ec-77f0-4bd4-afb8-b3fb4596b225')), RunInfo(run_id=UUID('011d3a71-8ce9-4068-83a3-d5feccb7f5ee'))]\n"
     ]
    }
   ],
   "source": [
    "# Approach 1: iterate through all questions one at a time\n",
    "qa = [\n",
    "    {\"question\": \"What is the capital city of France?\"},\n",
    "    {\"question\": \"What is the largest mammal on Earth?\"},\n",
    "    {\"question\": \"Which gas is most abundant in Earth's atmosphere?\"},\n",
    "    {\"question\": \"What color is a ripe banana?\"},\n",
    "]\n",
    "res = llm_chain.generate(qa)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1. Paris 2. giraffe 3. nitrogen 4. yellow 5. Jawaharlal Nehru 6. blue'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Approach 2: place all questions into a single prompt\n",
    "multi_template = \"\"\"Answer the following questions one at a time.\n",
    "\n",
    "Questions:\n",
    "{questions}\n",
    "\n",
    "Answers:\n",
    "\"\"\"\n",
    "long_prompt = PromptTemplate(template=multi_template, input_variables=[\"questions\"])\n",
    "\n",
    "llm_chain = LLMChain(prompt=long_prompt, llm=hub_llm)\n",
    "\n",
    "qs_str = (\n",
    "    \"1. What is the capital city of France?\\n\"\n",
    "    + \"2. What is the largest mammal on Earth?\\n\"\n",
    "    + \"3. Which gas is most abundant in Earth's atmosphere?\\n\"\n",
    "    + \"4. What color is a ripe banana?\\n\"\n",
    "    + \"5. Who was the first president of India?\\n\"\n",
    "    + \"6. What is the color of sky?\\n\"\n",
    ")\n",
    "response = llm_chain.run(questions=qs_str)\n",
    "response"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
