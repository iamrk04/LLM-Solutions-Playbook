{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LangChain: Memory\n",
    "\n",
    "By default, LLMs do not remember anything. But we can achieve this via:\n",
    "- ConversationBufferMemory\n",
    "- ConversationBufferWindowMemory\n",
    "- ConversationTokenBufferMemory\n",
    "- ConversationSummaryBufferMemory\n",
    "\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "_ = load_dotenv(find_dotenv())\n",
    "openai.api_type = os.environ.get(\"OPENAI_API_TYPE\")\n",
    "openai.api_base = os.environ.get(\"OPENAI_API_BASE\")\n",
    "openai.api_key = os.environ.get(\"OPENAI_API_KEY\")\n",
    "openai.api_version = os.environ.get(\"OPENAI_API_VERSION\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ConversationBufferMemory\n",
    "- Unlimited memory.\n",
    "- Stores all conversations.\n",
    "- Entire conversation is sent to LLM every time.\n",
    "- Can reach token limit quickly.\n",
    "- Costly as the tokens sent to LLM will be increasing with every ping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import AzureChatOpenAI\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationBufferMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat = AzureChatOpenAI(\n",
    "    deployment_name=\"gpt4\",\n",
    "    openai_api_version=\"2023-03-15-preview\",\n",
    "    temperature=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = ConversationBufferMemory()\n",
    "conversation = ConversationChain(llm=chat, memory=memory, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation.predict(input=\"Hi, my name is Alice?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation.predict(input=\"What's 1+1?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation.predict(input=\"What's my name?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory.buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modify memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory2 = ConversationBufferMemory()\n",
    "memory2.save_context({\"input\": \"Hi\"}, {\"output\": \"What's up?\"})\n",
    "memory2.buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory2.save_context({\"input\": \"Nothing much, just hanging.\"}, {\"output\": \"Cool.\"})\n",
    "memory2.buffer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ConversationBufferWindowMemory\n",
    "- Only keeps a window of memory.\n",
    "- `k` controls the window size.\n",
    "- prevents token limit from being reached.\n",
    "- not as costly as `ConversationBufferMemory` since number of tokens does not keep on growing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "\n",
    "memory = ConversationBufferWindowMemory(k=2)\n",
    "memory.save_context({\"input\": \"Hi\"}, {\"output\": \"What's up?\"})\n",
    "memory.save_context({\"input\": \"Nothing much, just hanging.\"}, {\"output\": \"Cool.\"})\n",
    "memory.save_context({\"input\": \"What's 1+1?\"}, {\"output\": \"2\"})\n",
    "memory.buffer  # shows entire buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory.load_memory_variables({})  # shows only last k items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation = ConversationChain(llm=chat, memory=memory, verbose=True)\n",
    "conversation.predict(input=\"What's my name?\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ConversationTokenBufferMemory\n",
    "- memory will limit the number of tokens saved.\n",
    "- more control on the cost since we can control the number of tokens sent to LLM.\n",
    "- need to also provide the `llm` along with `max_token_limit` (since, different llm count tokens differently)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationTokenBufferMemory\n",
    "\n",
    "memory = ConversationTokenBufferMemory(llm=chat, max_token_limit=40)\n",
    "memory.save_context({\"input\": \"Hi\"}, {\"output\": \"What's up?\"})\n",
    "memory.save_context({\"input\": \"Nothing much, just hanging.\"}, {\"output\": \"Cool.\"})\n",
    "memory.save_context({\"input\": \"What's 1+1?\"}, {\"output\": \"2\"})\n",
    "memory.buffer  # shows only last k tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory.load_memory_variables({})  # shows only last k tokens"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ConversationSummaryBufferMemory\n",
    "- stores the summary of the conversation in memory.\n",
    "- requires `llm` along with `max_token_limit`.\n",
    "- tokens are counted based on the `llm` provided.\n",
    "- if `max_token_limit` is reached, rest of the conversation is summarized using `llm` and stored in the memory as system message along with unsummarized last `max_token_limit` tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "\n",
    "memory = ConversationSummaryBufferMemory(llm=chat, max_token_limit=40)\n",
    "memory.save_context({\"input\": \"Hi\"}, {\"output\": \"What's up?\"})\n",
    "memory.save_context({\"input\": \"Nothing much, just hanging.\"}, {\"output\": \"Cool.\"})\n",
    "memory.save_context({\"input\": \"What's 1+1?\"}, {\"output\": \"2\"})\n",
    "memory.buffer  # shows only last k tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory.load_memory_variables(\n",
    "    {}\n",
    ")  # shows the summary along with the unsummarized last k tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation = ConversationChain(llm=chat, memory=memory, verbose=True)\n",
    "conversation.predict(input=\"What is the capital of India?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory.load_memory_variables({})  # shows the summary"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Memory Types\n",
    "- **Vector data memory**: Stores texts in a vector database and retrieves the most similar/relevant text to the input text.\n",
    "- **Entity memories**: Using an LLM, it remembers details about specific entities (like people, place, organization).\n",
    "\n",
    "> Note: You can also use multiple memory types together. You can also store the conversation in a database (such as key-value store or SQL)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
