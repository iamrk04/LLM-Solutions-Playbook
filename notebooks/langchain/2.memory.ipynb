{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LangChain: Memory\n",
    "\n",
    "By default, LLMs do not remember anything. But we can achieve this via:\n",
    "- ConversationBufferMemory\n",
    "- ConversationBufferWindowMemory\n",
    "- ConversationTokenBufferMemory\n",
    "- ConversationSummaryBufferMemory\n",
    "\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "_ = load_dotenv(find_dotenv())\n",
    "openai.api_type = os.environ.get(\"OPENAI_API_TYPE\")\n",
    "openai.api_base = os.environ.get(\"OPENAI_API_BASE\")\n",
    "openai.api_key = os.environ.get(\"OPENAI_API_KEY\")\n",
    "openai.api_version = os.environ.get(\"OPENAI_API_VERSION\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ConversationBufferMemory\n",
    "- Unlimited memory.\n",
    "- Stores all conversations.\n",
    "- Entire conversation is sent to LLM every time.\n",
    "- Can reach token limit quickly.\n",
    "- Costly as the tokens sent to LLM will be increasing with every ping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import AzureChatOpenAI\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationBufferMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat = AzureChatOpenAI(\n",
    "    deployment_name=\"gpt4\",\n",
    "    temperature=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = ConversationBufferMemory()\n",
    "conversation = ConversationChain(llm=chat, memory=memory, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: Hi, my name is Alice?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Hello Alice! It's nice to meet you. I'm an AI here to help you with any questions or topics you'd like to discuss. What can I help you with today?\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"Hi, my name is Alice?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hi, my name is Alice?\n",
      "AI: Hello Alice! It's nice to meet you. I'm an AI here to help you with any questions or topics you'd like to discuss. What can I help you with today?\n",
      "Human: What's 1+1?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'1+1 equals 2.'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"What's 1+1?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hi, my name is Alice?\n",
      "AI: Hello Alice! It's nice to meet you. I'm an AI here to help you with any questions or topics you'd like to discuss. What can I help you with today?\n",
      "Human: What's 1+1?\n",
      "AI: 1+1 equals 2.\n",
      "Human: What's my name?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Your name is Alice.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"What's my name?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Human: Hi, my name is Alice?\\nAI: Hello Alice! It's nice to meet you. I'm an AI here to help you with any questions or topics you'd like to discuss. What can I help you with today?\\nHuman: What's 1+1?\\nAI: 1+1 equals 2.\\nHuman: What's my name?\\nAI: Your name is Alice.\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': \"Human: Hi, my name is Alice?\\nAI: Hello Alice! It's nice to meet you. I'm an AI here to help you with any questions or topics you'd like to discuss. What can I help you with today?\\nHuman: What's 1+1?\\nAI: 1+1 equals 2.\\nHuman: What's my name?\\nAI: Your name is Alice.\"}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modify memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Human: Hi\\nAI: What's up?\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory2 = ConversationBufferMemory()\n",
    "memory2.save_context({\"input\": \"Hi\"}, {\"output\": \"What's up?\"})\n",
    "memory2.buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Human: Hi\\nAI: What's up?\\nHuman: Nothing much, just hanging.\\nAI: Cool.\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory2.save_context({\"input\": \"Nothing much, just hanging.\"}, {\"output\": \"Cool.\"})\n",
    "memory2.buffer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ConversationBufferWindowMemory\n",
    "- Only keeps a window of memory.\n",
    "- `k` controls the window size.\n",
    "- prevents token limit from being reached.\n",
    "- not as costly as `ConversationBufferMemory` since number of tokens does not keep on growing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='Hi', additional_kwargs={}, example=False),\n",
       " AIMessage(content=\"What's up?\", additional_kwargs={}, example=False),\n",
       " HumanMessage(content='Nothing much, just hanging.', additional_kwargs={}, example=False),\n",
       " AIMessage(content='Cool.', additional_kwargs={}, example=False),\n",
       " HumanMessage(content=\"What's 1+1?\", additional_kwargs={}, example=False),\n",
       " AIMessage(content='2', additional_kwargs={}, example=False)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "\n",
    "memory = ConversationBufferWindowMemory(k=2)\n",
    "memory.save_context({\"input\": \"Hi\"}, {\"output\": \"What's up?\"})\n",
    "memory.save_context({\"input\": \"Nothing much, just hanging.\"}, {\"output\": \"Cool.\"})\n",
    "memory.save_context({\"input\": \"What's 1+1?\"}, {\"output\": \"2\"})\n",
    "memory.buffer  # shows entire buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': \"Human: Nothing much, just hanging.\\nAI: Cool.\\nHuman: What's 1+1?\\nAI: 2\"}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})  # shows only last k conversations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Nothing much, just hanging.\n",
      "AI: Cool.\n",
      "Human: What's 1+1?\n",
      "AI: 2\n",
      "Human: What's my name?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"I'm sorry, I don't know your name.\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation = ConversationChain(llm=chat, memory=memory, verbose=True)\n",
    "conversation.predict(input=\"What's my name?\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ConversationTokenBufferMemory\n",
    "- memory will limit the number of tokens saved.\n",
    "- more control on the cost since we can control the number of tokens sent to LLM.\n",
    "- need to also provide the `llm` along with `max_token_limit` (since, different llm count tokens differently)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='Nothing much, just hanging.', additional_kwargs={}, example=False),\n",
       " AIMessage(content='Cool.', additional_kwargs={}, example=False),\n",
       " HumanMessage(content=\"What's 1+1?\", additional_kwargs={}, example=False),\n",
       " AIMessage(content='2', additional_kwargs={}, example=False)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.memory import ConversationTokenBufferMemory\n",
    "\n",
    "memory = ConversationTokenBufferMemory(llm=chat, max_token_limit=40)\n",
    "memory.save_context({\"input\": \"Hi\"}, {\"output\": \"What's up?\"})\n",
    "memory.save_context({\"input\": \"Nothing much, just hanging.\"}, {\"output\": \"Cool.\"})\n",
    "memory.save_context({\"input\": \"What's 1+1?\"}, {\"output\": \"2\"})\n",
    "memory.buffer  # shows only last k tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': \"Human: Nothing much, just hanging.\\nAI: Cool.\\nHuman: What's 1+1?\\nAI: 2\"}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})  # shows only last k tokens"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ConversationSummaryBufferMemory\n",
    "- stores the summary of the conversation in memory.\n",
    "- requires `llm` along with `max_token_limit`.\n",
    "- tokens are counted based on the `llm` provided.\n",
    "- if `max_token_limit` is reached, rest of the conversation is summarized using `llm` and stored in the memory as system message along with unsummarized last `max_token_limit` tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='Nothing much, just hanging.', additional_kwargs={}, example=False),\n",
       " AIMessage(content='Cool.', additional_kwargs={}, example=False),\n",
       " HumanMessage(content=\"What's 1+1?\", additional_kwargs={}, example=False),\n",
       " AIMessage(content='2', additional_kwargs={}, example=False)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "\n",
    "memory = ConversationSummaryBufferMemory(llm=chat, max_token_limit=40)\n",
    "memory.save_context({\"input\": \"Hi\"}, {\"output\": \"What's up?\"})\n",
    "memory.save_context({\"input\": \"Nothing much, just hanging.\"}, {\"output\": \"Cool.\"})\n",
    "memory.save_context({\"input\": \"What's 1+1?\"}, {\"output\": \"2\"})\n",
    "memory.buffer  # shows only last k tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': \"System: The human greets the AI, and the AI responds by asking what's up.\\nHuman: Nothing much, just hanging.\\nAI: Cool.\\nHuman: What's 1+1?\\nAI: 2\"}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables(\n",
    "    {}\n",
    ")  # shows the summary along with the unsummarized last k tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "System: The human greets the AI, and the AI responds by asking what's up.\n",
      "AI: 2\n",
      "Human: What is the capital of India?\n",
      "AI: The capital of India is New Delhi.\n",
      "Human: What is the capital of India?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The capital of India is New Delhi.'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation = ConversationChain(llm=chat, memory=memory, verbose=True)\n",
    "conversation.predict(input=\"What is the capital of India?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': \"System: The human greets the AI, and the AI responds by asking what's up. The AI answers the human's question about the capital of India, which is New Delhi.\\nHuman: What is the capital of India?\\nAI: The capital of India is New Delhi.\"}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})  # shows the summary"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Memory Types\n",
    "- **Vector data memory**: Stores texts in a vector database and retrieves the most similar/relevant text to the input text.\n",
    "- **Entity memories**: Using an LLM, it remembers details about specific entities (like people, place, organization).\n",
    "\n",
    "> Note: You can also use multiple memory types together. You can also store the conversation in a database (such as key-value store or SQL)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
