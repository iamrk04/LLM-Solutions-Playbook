{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Splitters\n",
    "LLMs have a maximum prompt size, preventing them from feeding entire documents. This makes it crucial to divide documents into smaller parts, and Text Splitters prove to be extremely useful in achieving this. Text Splitters help break down large text documents into smaller, more digestible pieces that language models can process more effectively.\n",
    "\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pros:\n",
    "- **Reduced hallucination**: By providing a source document, the LLM is more likely to generate content based on the given information, reducing the chances of creating false or irrelevant information.\n",
    "- **Increased accuracy**: With a reliable source document, the LLM can generate more accurate answers, especially in use cases where accuracy is crucial.\n",
    "- **Verifiable information**: Users can cross-check the generated content with the source document to ensure the information is accurate and reliable.\n",
    "\n",
    "Cons:\n",
    "- **Limited scope**: Relying on a single document may limit the scope of the generated content, as the LLM will only have access to the information provided in the document.\n",
    "- **Dependence on document quality**: The accuracy of the generated content heavily depends on the quality and reliability of the source document. The LLM will likely generate incorrect or misleading content if the document contains inaccurate or biased information.\n",
    "- **Inability to eliminate hallucination completely**: Although providing a document as a base reduces the chances of hallucination, it does not guarantee that the LLM will never generate false or irrelevant information."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Character Text Splitter\n",
    "This type of splitter can be used in various scenarios where you must split long text pieces into smaller, semantically meaningful chunks. The splitter allows you to customize the chunking process along two axes - chunk size and chunk overlap - to balance the trade-offs between splitting the text into manageable pieces and preserving semantic context between chunks.\n",
    "\n",
    "> Note: Only one separator can be specified for this splitter. If chunk_size of the splitted doc is more than the specified chunk_size, and the separator is not found, then further splitting won't be done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "loader = PyPDFLoader(\"../../data/WritingArticleSummary.pdf\")\n",
    "pages = loader.load()\n",
    "len(pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You have 10 documents\n",
      "Preview:\n",
      "1 \n",
      " \n",
      "Academic Skills, Trent University www.trentu.ca/academicskills \n",
      "Peterborough, ON Canada © 2014 \n",
      " Writing Article Summaries \n",
      " \n",
      " \n",
      "Understanding Article Summaries \n",
      "An article summary is a short, focused paper about one scholarly \n",
      "article. This paper is informed by critical reading of an article. For \n",
      "argumentative articles, the summary identifies, explains, and \n",
      "analyses the thesis and supporting arguments; for empirica l articles, \n",
      "the summary identifies, explains, and analyses the research \n",
      "questions, methods, and findings. \n",
      "Although article summaries are often short and rarely account for a \n",
      "large portion of your grade, they are a strong indicator of your \n",
      "reading and writ ing skills. Professors ask you to write article \n",
      "summaries to help you to develop essential skills in critical reading, \n",
      "summarizing, and clear, organized writing. Furthermore, an article \n",
      "summary requires you to read a scholarly article quite closely, which \n",
      "provides a useful introduction to the conventions of\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "text_splitter = CharacterTextSplitter(separator=\" \", chunk_size=1000, chunk_overlap=20)\n",
    "texts = text_splitter.split_documents(pages)\n",
    "\n",
    "print(f\"You have {len(texts)} documents\")\n",
    "\n",
    "print(\"Preview:\")\n",
    "print(texts[0].page_content)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Note: No universal approach for chunking text will fit all scenarios. Finding the best chunk size means going through a few steps. First, clean up your data by getting rid of anything that's not needed, like HTML tags from websites. Then, pick a few different chunk sizes to test. The best size will depend on what kind of data you're working with and the model you're using. Finally, test out how well each size works by running some queries and comparing the results. You might need to try a few different sizes before finding the best one. This process might take some time, but getting the best results from your project is worth it."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Recursive Character Text Splitter\n",
    "The Recursive Character Text Splitter is a text splitter designed to split the text into chunks based on a list of characters provided. It attempts to split text using the characters from a list in order until the resulting chunks are small enough. By default, the list of characters used for splitting is `[\"\\n\\n\", \"\\n\", \" \", \"\"]`, which tries to keep paragraphs, sentences, and words together as long as possible, as they are generally the most semantically related pieces of text.\n",
    "\n",
    "> Note: It extends the Character Text Splitter. If the chunk_size of the splitted doc is more than the specified chunk_size, then the splitter will try to split the text using the characters from the list in order until the resulting chunks are small enough."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "loader = PyPDFLoader(\"../../data/WritingArticleSummary.pdf\")\n",
    "pages = loader.load()\n",
    "len(pages)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Argument `length_function` is powerful, provide a  custom function when you have your own logic for calculating length of chunk. To use a token counter, you can create a custom function that calculates the number of tokens in a given text and pass it as the `length_function` parameter. This will ensure that your text splitter calculates the length of chunks based on the number of tokens instead of the number of characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "Preview:\n",
      "1 \n",
      " \n",
      "Academic Skills, Trent University    www.trentu.ca/academicskills  \n",
      "Peterborough, ON Canada                     © 2014  \n",
      "  Writing Article Summaries  \n",
      " \n",
      " \n",
      "Understanding Article Summaries  \n",
      "An article summary is a short, focused paper about one scholarly \n",
      "article. This paper is informed by critical reading of an article. For \n",
      "argumentative articles, the summary identifies, explains, and \n",
      "analyses the thesis and supporting arguments; for empirica l articles, \n",
      "the summary identifies, explains, and analyses the research \n",
      "questions, methods, and findings.  \n",
      "Although article summaries are often short and rarely account for a \n",
      "large portion of your grade, they are a strong indicator of your \n",
      "reading and writ ing skills. Professors ask you to write article \n",
      "summaries to help you to develop essential skills in critical reading, \n",
      "summarizing, and clear, organized writing. Furthermore, an article \n",
      "summary requires you to read a scholarly article quite closely, which\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=10,\n",
    "    length_function=len,\n",
    ")\n",
    "# When you have text as a string (by directly reading a file) intead of documents\n",
    "# texts = text_splitter.create_documents([text])\n",
    "texts = text_splitter.split_documents(pages)\n",
    "print(len(texts))\n",
    "\n",
    "print(\"Preview:\")\n",
    "print(texts[0].page_content)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. NLTK Text Splitter\n",
    "The `NLTKTextSplitter` in LangChain is an implementation of a text splitter that uses the Natural Language Toolkit (NLTK) library to split text based on tokenizers. The goal is to split long texts into smaller chunks without breaking the structure of sentences and paragraphs.\n",
    "\n",
    "However, the `NLTKTextSplitter` is not specifically designed to handle word segmentation in English sentences without spaces. For this purpose, you can use alternative libraries like pyenchant or word segment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "loader = PyPDFLoader(\"../../data/WritingArticleSummary.pdf\")\n",
    "pages = loader.load()\n",
    "len(pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n",
      "Preview:\n",
      "1 \n",
      " \n",
      "Academic Skills, Trent University    www.trentu.ca/academicskills  \n",
      "Peterborough, ON Canada                     © 2014  \n",
      "  Writing Article Summaries  \n",
      " \n",
      " \n",
      "Understanding Article Summaries  \n",
      "An article summary is a short, focused paper about one scholarly \n",
      "article.\n",
      "\n",
      "This paper is informed by critical reading of an article.\n",
      "\n",
      "For \n",
      "argumentative articles, the summary identifies, explains, and \n",
      "analyses the thesis and supporting arguments; for empirica l articles, \n",
      "the summary identifies, explains, and analyses the research \n",
      "questions, methods, and findings.\n",
      "\n",
      "Although article summaries are often short and rarely account for a \n",
      "large portion of your grade, they are a strong indicator of your \n",
      "reading and writ ing skills.\n",
      "\n",
      "Professors ask you to write article \n",
      "summaries to help you to develop essential skills in critical reading, \n",
      "summarizing, and clear, organized writing.\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import NLTKTextSplitter\n",
    "\n",
    "text_splitter = NLTKTextSplitter(chunk_size=1000)\n",
    "texts = text_splitter.split_documents(pages)\n",
    "print(len(texts))\n",
    "\n",
    "print(\"Preview:\")\n",
    "print(texts[0].page_content)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. `SpacyTextSplitter`\n",
    "The `SpacyTextSplitter` helps split large text documents into smaller chunks based on a specified size. This is useful for better management of large text inputs. It's important to note that the `SpacyTextSplitter` is an alternative to NLTK-based sentence splitting. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "loader = PyPDFLoader(\"../../data/WritingArticleSummary.pdf\")\n",
    "pages = loader.load()\n",
    "len(pages)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you see the following error on executing the cell below:\n",
    "`OSError: [E050] Can't find model 'en_core_web_sm'. It doesn't seem to be a Python package or a valid path to a data directory.`\n",
    "\n",
    "Then run the following command in your terminal:\n",
    "`python -m spacy download en_core_web_sm`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "Preview:\n",
      "1 \n",
      " \n",
      "Academic Skills, Trent University    www.trentu.ca/academicskills  \n",
      "Peterborough, ON Canada                     © 2014  \n",
      "  \n",
      "\n",
      "Writing Article Summaries  \n",
      " \n",
      " \n",
      "Understanding Article Summaries  \n",
      "An article summary is a short, focused paper about one scholarly \n",
      "article.\n",
      "\n",
      "This paper is informed by critical reading of an article.\n",
      "\n",
      "For \n",
      "argumentative articles, the summary identifies, explains, and \n",
      "analyses the thesis and supporting arguments; for empirica l articles, \n",
      "the summary identifies, explains, and analyses the research \n",
      "questions, methods, and findings.  \n",
      "\n",
      "\n",
      "Although article summaries are often short and rarely account for a \n",
      "large portion of your grade, they are a strong indicator of your \n",
      "reading and writ ing skills.\n",
      "\n",
      "Professors ask you to write article \n",
      "summaries to help you to develop essential skills in critical reading, \n",
      "summarizing, and clear, organized writing.\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import SpacyTextSplitter\n",
    "\n",
    "text_splitter = SpacyTextSplitter(chunk_size=1000, chunk_overlap=20)\n",
    "\n",
    "texts = text_splitter.split_documents(pages)\n",
    "\n",
    "print(len(texts))\n",
    "\n",
    "print(\"Preview:\")\n",
    "print(texts[0].page_content)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. `MarkdownTextSplitter`\n",
    "The `MarkdownTextSplitter` is designed to split text written using Markdown languages like headers, code blocks, or dividers. It is implemented as a simple subclass of `RecursiveCharacterSplitter` with Markdown-specific separators. By default, these separators are determined by the Markdown syntax, but they can be customized by providing a list of characters during the initialization of the MarkdownTextSplitter instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(page_content='# \\n\\n# Welcome to My Blog!', metadata={}), Document(page_content='## Introduction', metadata={}), Document(page_content='Hello everyone! My name is **John Doe** and I am a _software developer_. I specialize in Python,', metadata={}), Document(page_content='Java, and JavaScript.', metadata={}), Document(page_content=\"Here's a list of my favorite programming languages:\\n\\n1. Python\\n2. JavaScript\\n3. Java\", metadata={}), Document(page_content='You can check out some of my projects on [GitHub](https://github.com).', metadata={}), Document(page_content='## About this Blog', metadata={}), Document(page_content=\"In this blog, I will share my journey as a software developer. I'll post tutorials, my thoughts on\", metadata={}), Document(page_content='the latest technology trends, and occasional book reviews.', metadata={}), Document(page_content=\"Here's a small piece of Python code to say hello:\", metadata={}), Document(page_content='\\\\``` python\\ndef say_hello(name):\\n    print(f\"Hello, {name}!\")\\n\\nsay_hello(\"John\")\\n\\\\', metadata={}), Document(page_content='```\\n\\nStay tuned for more updates!', metadata={}), Document(page_content='## Contact Me', metadata={}), Document(page_content='Feel free to reach out to me on [Twitter](https://twitter.com) or send me an email at', metadata={}), Document(page_content='johndoe@email.com.', metadata={})]\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import MarkdownTextSplitter\n",
    "\n",
    "markdown_text = \"\"\"\n",
    "# \n",
    "\n",
    "# Welcome to My Blog!\n",
    "\n",
    "## Introduction\n",
    "Hello everyone! My name is **John Doe** and I am a _software developer_. I specialize in Python, Java, and JavaScript.\n",
    "\n",
    "Here's a list of my favorite programming languages:\n",
    "\n",
    "1. Python\n",
    "2. JavaScript\n",
    "3. Java\n",
    "\n",
    "You can check out some of my projects on [GitHub](https://github.com).\n",
    "\n",
    "## About this Blog\n",
    "In this blog, I will share my journey as a software developer. I'll post tutorials, my thoughts on the latest technology trends, and occasional book reviews.\n",
    "\n",
    "Here's a small piece of Python code to say hello:\n",
    "\n",
    "\\``` python\n",
    "def say_hello(name):\n",
    "    print(f\"Hello, {name}!\")\n",
    "\n",
    "say_hello(\"John\")\n",
    "\\```\n",
    "\n",
    "Stay tuned for more updates!\n",
    "\n",
    "## Contact Me\n",
    "Feel free to reach out to me on [Twitter](https://twitter.com) or send me an email at johndoe@email.com.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "markdown_splitter = MarkdownTextSplitter(chunk_size=100, chunk_overlap=0)\n",
    "docs = markdown_splitter.create_documents([markdown_text])\n",
    "\n",
    "print(docs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. `TokenTextSplitter`\n",
    "The main advantage of using `TokenTextSplitter` over other text splitters, like `CharacterTextSplitter`, is that it respects the token boundaries, ensuring that the chunks do not split tokens in the middle. This can be particularly helpful in maintaining the semantic integrity of the text when working with language models and embeddings.\n",
    "\n",
    "This type of splitter breaks down raw text strings into smaller pieces by initially converting the text into BPE (Byte Pair Encoding) tokens, and subsequently dividing these tokens into chunks. It then reassembles the tokens within each chunk back into text.\n",
    "\n",
    "One potential drawback of using TokenTextSplitter is that it may require additional computation when converting text to BPE tokens and back. If you need a faster and simpler text-splitting method, you might consider using CharacterTextSplitter, which directly splits the text based on character count, offering a more straightforward approach to text segmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "loader = PyPDFLoader(\"../../data/WritingArticleSummary.pdf\")\n",
    "pages = loader.load()\n",
    "len(pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "Preview:\n",
      "1 \n",
      " \n",
      "Academic Skills, Trent University    www.trentu.ca/academicskills  \n",
      "Peterborough, ON Canada                     © 2014  \n",
      "  Writing Article Summaries  \n",
      " \n",
      " \n",
      "Understanding Article Summaries  \n",
      "An article summary is a short, focused paper about one scholarly \n",
      "article. This paper is informed by critical reading of an article. For \n",
      "argumentative articles, the summary identifies, explains, and \n",
      "analyses the thesis and supporting arguments; for empirica l articles, \n",
      "the summary identifies, explains, and analyses the research \n",
      "questions, methods, and findings.  \n",
      "Although article summaries are often short and rarely account for a \n",
      "large portion of your grade, they are a strong indicator of your \n",
      "reading and writ ing skills. Professors ask you to write article \n",
      "summaries to help you to develop essential skills in critical reading, \n",
      "summarizing, and clear, organized writing. Furthermore, an article \n",
      "summary requires you to read a scholarly article quite closely, which \n",
      "provides a useful introduction to the conventions of writing in your \n",
      "discipline (e.g. Political Studies, Biology, or Anthropology).  \n",
      "Common Problems in Article Summaries  \n",
      "The most common problem that students have when writing an \n",
      "article summary is that the y misunderstand the goal of the \n",
      "assignment. In an article summary, your job is to write about the \n",
      "article,  not about the actual topic of the article. For example, if you \n",
      "are summarizing Smith’s a rticle about the causes of the B ubonic \n",
      "plague in Europe, your  summary should be about Smith’s article: \n",
      "What does she want to find out about the plague? What evidence \n",
      "does she use? What is her argument? You are not writing a pa per \n",
      "about the actual causes of B ubonic plague in Europe.  Further, a s a part of critical r eading, you will often consider your \n",
      "own position on a topic or an argument; it is tempting to include an \n",
      "assessment or opinion about the thesis or findings, but this is not \n",
      "the goal of an\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import TokenTextSplitter\n",
    "\n",
    "# Initialize the TokenTextSplitter with desired chunk size and overlap\n",
    "text_splitter = TokenTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "\n",
    "# Split into smaller chunks\n",
    "texts = text_splitter.split_documents(pages)\n",
    "\n",
    "print(len(texts))\n",
    "\n",
    "print(\"Preview:\")\n",
    "print(texts[0].page_content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
